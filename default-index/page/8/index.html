<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" integrity="sha256-wiz7ZSCn/btzhjKDQBms9Hx4sSeUYsDrTLg7roPstac=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"skk1faker.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":true,"version":"8.19.2","exturl":false,"sidebar":{"position":"left","width":240,"display":"always","padding":18,"offset":12},"copycode":{"enable":true,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":false,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="登峰造极">
<meta property="og:type" content="website">
<meta property="og:title" content="skk1faker 笔记">
<meta property="og:url" content="https://skk1faker.github.io/default-index/page/8/index.html">
<meta property="og:site_name" content="skk1faker 笔记">
<meta property="og:description" content="登峰造极">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="skk1faker">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://skk1faker.github.io/default-index/page/8/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"default-index/page/8/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>skk1faker 笔记</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">skk1faker 笔记</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>文章</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-navigation"><a href="/navigation" rel="section"><i class="fa fa-user fa-fw"></i>导航</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="skk1faker"
      src="/images/acm-icpc.png">
  <p class="site-author-name" itemprop="name">skk1faker</p>
  <div class="site-description" itemprop="description">登峰造极</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">77</span>
          <span class="site-state-item-name">文章</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">59</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/skk1faker" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;skk1faker" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:skk1faker@163.com" title="E-Mail → mailto:skk1faker@163.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://stackoverflow.com/" title="StackOverflow → https:&#x2F;&#x2F;stackoverflow.com" rel="noopener me" target="_blank"><i class="fab fa-stack-overflow fa-fw"></i>StackOverflow</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://skk1faker.github.io/2023/11/07/cf1006F-Xor-Paths-%E5%8F%8C%E5%90%91%E6%90%9C%E7%B4%A2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/acm-icpc.png">
      <meta itemprop="name" content="skk1faker">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="skk1faker 笔记">
      <meta itemprop="description" content="登峰造极">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | skk1faker 笔记">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/11/07/cf1006F-Xor-Paths-%E5%8F%8C%E5%90%91%E6%90%9C%E7%B4%A2/" class="post-title-link" itemprop="url">cf1606-Xor-Paths-双向搜索</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-11-07 18:27:46" itemprop="dateCreated datePublished" datetime="2023-11-07T18:27:46+08:00">2023-11-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-02-22 12:04:39" itemprop="dateModified" datetime="2025-02-22T12:04:39+08:00">2025-02-22</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="题意">题意</h1>
<p>在一个$ 20 <span
class="math inline">\(的网格上，每个格子上有一些权值，选出一个从\)</span>(1,1)<span
class="math inline">\(点到\)</span>(n,m)$点的路径，这条路径满足：</p>
<ul>
<li>路径的当前点到下一个点的运动方向只能是坐标值增加的方向，即<span
class="math inline">\((i , j)\)</span>可以移动到<span
class="math inline">\((i,j+1)、(i + 1,j)\)</span></li>
<li>路径上权值异或和为k，其中<span class="math inline">\(0 \le k  \le
10^{18}\)</span></li>
</ul>
<p>现求满足条件的路径数</p>
<h1 id="题解">题解</h1>
<p>如果k值很小的话，那么一个很简单的一个dp方案就出现了 <span
class="math display">\[dp[i][j][k_1] = dp[i - 1][j][k_1 \oplus k] +
dp[i][j - 1][k \oplus k_1]\]</span>
但是观察k的取值范围，如果真的使用这个转移方程，只会导致空间开的特别大。所以这里使用stl中的map，<code>map&lt;ll,ll&gt;dp[][]</code>来完成这个转移。</p>
<p>当然到这步其实还没有完全结束，如果使用这种转移，k的数量一样会很多，我们假设每种路径的异或值都不一样，那么每个<span
class="math inline">\(dp[x][y]\)</span>中的结果存储元素个数应该是<span
class="math inline">\(C_{x + y - 2}^{x - 1}\)</span>种，$C_{20}^{10} =
$为了解决这个问题可以使用双向搜索。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*================================================================</span></span><br><span class="line"><span class="comment">*   Copyright (C) 2023 Wangxinpeng. All rights reserved.</span></span><br><span class="line"><span class="comment">*</span></span><br><span class="line"><span class="comment">*   filename：    cf1006f.cpp</span></span><br><span class="line"><span class="comment">*   username:     skt1faker</span></span><br><span class="line"><span class="comment">*   create time:  09:37  2023.11.07</span></span><br><span class="line"><span class="comment">    email:        skk1faker@163.com</span></span><br><span class="line"><span class="comment">*   descripe:</span></span><br><span class="line"><span class="comment">*</span></span><br><span class="line"><span class="comment">================================================================*/</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="meta">#<span class="keyword">define</span> ll long long</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> DEBUG0</span></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> mod1 = <span class="number">1e9</span> + <span class="number">7</span>;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> mod2 = <span class="number">998244353</span>;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> maxx = <span class="number">25</span>;</span><br><span class="line">map&lt;ll,ll&gt; dp[<span class="number">2</span>][maxx][maxx];</span><br><span class="line">ll v[maxx][maxx];</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="type">int</span> n, m;</span><br><span class="line">  ll k;</span><br><span class="line">  cin &gt;&gt; n &gt;&gt; m &gt;&gt; k;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; m; j++) &#123;</span><br><span class="line">      <span class="built_in">scanf</span>(<span class="string">&quot;%lld&quot;</span>, &amp;v[i][j]);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="type">int</span> split_posi = (n - <span class="number">1</span>);</span><br><span class="line">  dp[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>][v[<span class="number">0</span>][<span class="number">0</span>]] = <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; m; j++) &#123;</span><br><span class="line">      <span class="keyword">if</span>(i == <span class="number">0</span> &amp;&amp; j == <span class="number">0</span> || (i + j) &gt; split_posi)<span class="keyword">continue</span>;</span><br><span class="line">      <span class="keyword">if</span>(i - <span class="number">1</span> &gt;= <span class="number">0</span>)&#123;</span><br><span class="line">        <span class="keyword">for</span> (map&lt;ll,ll&gt;::iterator iter = dp[<span class="number">0</span>][i - <span class="number">1</span>][j].<span class="built_in">begin</span>();iter != dp[<span class="number">0</span>][i - <span class="number">1</span>][j].<span class="built_in">end</span>(); iter++) &#123;</span><br><span class="line">          dp[<span class="number">0</span>][i][j][iter-&gt;first ^ v[i][j]] += iter-&gt;second;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span>(j - <span class="number">1</span> &gt;= <span class="number">0</span>)&#123;</span><br><span class="line">        <span class="keyword">for</span> (map&lt;ll,ll&gt;::iterator iter = dp[<span class="number">0</span>][i][j - <span class="number">1</span>].<span class="built_in">begin</span>();iter != dp[<span class="number">0</span>][i][j - <span class="number">1</span>].<span class="built_in">end</span>(); iter++) &#123;</span><br><span class="line">          dp[<span class="number">0</span>][i][j][iter-&gt;first ^ v[i][j]] += iter-&gt;second;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span>(n + m - <span class="number">2</span> == split_posi)</span><br><span class="line">    dp[<span class="number">1</span>][n - <span class="number">1</span>][m - <span class="number">1</span>][<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">else</span> </span><br><span class="line">    dp[<span class="number">1</span>][n - <span class="number">1</span>][m - <span class="number">1</span>][v[n - <span class="number">1</span>][m - <span class="number">1</span>]] = <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> i = n - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i--) &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> j = m - <span class="number">1</span>; j &gt;= <span class="number">0</span>; j--) &#123;</span><br><span class="line">      <span class="keyword">if</span>(i == n - <span class="number">1</span> &amp;&amp; j == m - <span class="number">1</span> || (i + j) &lt; split_posi)<span class="keyword">continue</span>;</span><br><span class="line">      <span class="keyword">if</span>(i + <span class="number">1</span> &lt; n)&#123;</span><br><span class="line">        <span class="keyword">for</span> (map&lt;ll,ll&gt;::iterator iter = dp[<span class="number">1</span>][i + <span class="number">1</span>][j].<span class="built_in">begin</span>();iter != dp[<span class="number">1</span>][i + <span class="number">1</span>][j].<span class="built_in">end</span>(); iter++) &#123;</span><br><span class="line">          <span class="keyword">if</span>(i + j == split_posi)</span><br><span class="line">            dp[<span class="number">1</span>][i][j][iter-&gt;first] += iter-&gt;second;</span><br><span class="line">          <span class="keyword">else</span> </span><br><span class="line">            dp[<span class="number">1</span>][i][j][iter-&gt;first ^ v[i][j]] += iter-&gt;second;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span>(j + <span class="number">1</span> &lt; m)&#123;</span><br><span class="line">        <span class="keyword">for</span> (map&lt;ll,ll&gt;::iterator iter = dp[<span class="number">1</span>][i][j + <span class="number">1</span>].<span class="built_in">begin</span>();iter != dp[<span class="number">1</span>][i][j + <span class="number">1</span>].<span class="built_in">end</span>(); iter++) &#123;</span><br><span class="line">          <span class="keyword">if</span>(i + j == split_posi)</span><br><span class="line">            dp[<span class="number">1</span>][i][j][iter-&gt;first] += iter-&gt;second;</span><br><span class="line">          <span class="keyword">else</span> </span><br><span class="line">            dp[<span class="number">1</span>][i][j][iter-&gt;first ^ v[i][j]] += iter-&gt;second;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  ll ans = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>;i &lt; n;i++)&#123;</span><br><span class="line">    <span class="type">int</span> j = split_posi - i;</span><br><span class="line">    <span class="keyword">if</span>(!(<span class="number">0</span> &lt;= j &amp;&amp; j &lt; m))<span class="keyword">continue</span>;</span><br><span class="line">    <span class="keyword">for</span>(map&lt;ll,ll&gt;::iterator iter = dp[<span class="number">1</span>][i][j].<span class="built_in">begin</span>();iter != dp[<span class="number">1</span>][i][j].<span class="built_in">end</span>();iter++)&#123;</span><br><span class="line">      ans += iter-&gt;second * dp[<span class="number">0</span>][i][j][(k ^ (iter-&gt;first))];</span><br><span class="line">      <span class="comment">//cout&lt;&lt;(k ^ iter-&gt;first)&lt;&lt;&#x27; &#x27;&lt;&lt;dp[0][i][j][(k ^ (iter-&gt;first))]&lt;&lt;endl;;</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  cout&lt;&lt;ans&lt;&lt;endl;</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://skk1faker.github.io/2023/10/27/paper-graduate/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/acm-icpc.png">
      <meta itemprop="name" content="skk1faker">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="skk1faker 笔记">
      <meta itemprop="description" content="登峰造极">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | skk1faker 笔记">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/10/27/paper-graduate/" class="post-title-link" itemprop="url">paper_graduate</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-10-27 20:04:09" itemprop="dateCreated datePublished" datetime="2023-10-27T20:04:09+08:00">2023-10-27</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-02-22 12:04:39" itemprop="dateModified" datetime="2025-02-22T12:04:39+08:00">2025-02-22</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/132561405">参考1</a></p>
<p>[]</p>
<h1 id="摘要">摘要</h1>
<p>暗光增强在多种领域有所应用，如视频监控，自动驾驶等，尤其是智能手机，其光圈较小、实时处理要求高、内存较小，在智能手机上实现暗光增强功能具有很大的挑战性。</p>
<h1 id="介绍">介绍</h1>
<p>近些年，深度神经网络越来越多的用于一些边缘设备上，一些边缘设备仅能提供较低的计算能力，并且在功耗和散热上有一些限制，需要一种算法来解决这样的问题，量化成为部署算法的主要方法之一。
## 量化国内外发展现状
在最近几年，由于工业界的需要网络量化发展的迅速，从最开始的在精度要求低的图像检测、识别任务上向精度要求高的任务开始过渡。工业界和学术界涌现了各种各样的量化方案，从2，3bit的低位量化到8bit量化，再到后来的多位量化，以及非均匀的对数量化。不同量化方式计算方法不同，这也促进了一些AI芯片的发展，现在的一些芯片可以很好的支持多种低位整数运算Imagination推出两款AI芯片IP内核AX2185与AX2145，能够支持16bit～4bit位宽，主打手机、智能监控、汽车等市场。量化方法已经在学术界给出了大量方案。
Guo,Zhang等人[1]提出了混合精度量化的相对与int8等固定精度量化来说主要问题在与位数选择，神经网络结构（NAS）方法用于网络结构的搜索，我们可以将量化位数当作一个模型结构参数，通过NAS方法搜索最佳网络,但是这种方法主要是计算量大，收敛速度和计算所需资源都极大。Yu,Han等人[2]借鉴了NAS搜索方法，但是减少了NAS的计算代价，通过为每种量化方式加权并将权重使用网络进行训练，最终权重最大所对应的网络位数即为选择位数，虽然计算量减小了，但是目前仅在图像识别和检测任务上进行了测试。H.Yang,L.Duan等人[3]提出一种新颖的方法，将量化位数大小转为二进制使之每一位成为可训练的变量，使得量化位数可通过网络优化得到。本方法很好的计算了网络权重层的量化位数，但是却未能对权重层输出的特征进行量化。Z.Dong,Z.Yao等人[4]提出使用海森矩阵统计的方式进行量化位数计算的方法，通过计算某层参数量化后平均意义上的误差来选择某层的量化位数。方法偏向于数学理论，但是其衡量量化误差仅从参数层面，量化方法层面没有考虑。R.Gong,X.Liu等人[5]提出一种使用tanh拟合取整函数的方法，使得反向传播不再使用传统的STE（straight-through
estimator）来进行估计损失，网络训练出来的参数更加合理，文中仅对固定为数量化进行了实验。C.Hong,H.Kim等人[6]提出了一种对于超分网络的量化方式，由于超分网络任务更加难，所以量化过程中需要降低量化对网络的损失，文中提出一个估计量化后网络损失的方法，将其作为正则项，利用网络自身训练减少损失。但文中提出的损失估计也仅仅是从权重量化损失层面上得到的，并没有考虑权重层的输出。李博闻[7]提出乐一种无训练数据量化方案，是得对于一些数据集不公开的网络也能进行量化</p>
<h2 id="蒸馏国内外发展现状">蒸馏国内外发展现状</h2>
<p>传统知识蒸馏仅仅达到了教师网络和学生网络之间的特征匹配，但是同一网络中不同特征之间的关系也是在网络推导时一个比较重要的信息，Yim等人（2017）年提出了关于基于关系的知识蒸馏方法的求解流程（FSP），其总结了特征图间的关系，使用两层特征之间的内积做计算，使用奇异值分解出特征图的关键信息进行知识蒸馏，Lee
et等2018年使用多个教师网络进行知识蒸馏，zhang和Peng（2018）使用多个教师网络的特征进行蒸馏，但是每个蒸馏的特征使用的是logits进行加权，形成一个蒸馏的多路计算图，每一个教师网络的蒸馏计算图根据logits中的权重不同而拥有不同的重要性。Lee和Song（2019）提出了基于多头图的知识蒸馏方法，通过多头注意力网络中的任意两个特征图之间的数据关系进行知识蒸馏。You等人2017，Park等人2019，Liu等人2019提出一种基于实例关系图蒸馏方法，关系图传递的知识包括层特征，层关系和跨层特征空间变换,Park等人提出一种基于流式学习的方案，通过特征嵌入对教师网络进行学习，保留了教师网络中渐层的特征相似性，Tung和Mori，2019提出一个相似保留知识的提取方法，将相似的激活值产生相似的知识层关系转移到学生网络中。Peng等人2019年提出一种蒸馏方法，其知识即包含相关的特征信息，又包含特征之间的相关性信息。
(只是蒸馏pdf 7,)</p>
<p>(知识蒸馏pdf 8,蒸馏方法) ## 量化蒸馏国内外发展现状 Yoojin Choi,
Jihwan Choi, Mostafa El-Khamy, Jungwon Lee; Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR) Workshops,
2020, pp. 710-711
Yoojin等人提出了无数据的对抗性知识蒸馏，它最小化了来自生成器的任何对抗性样本的教师和（量化）学生的输出之间的最大距离。但是主要工作还是在目标检测网络中</p>
<p>QKD: Quantization-aware Knowledge Distillation
Kim等人提出了量化与知识蒸馏相结合来解决模型压缩问题，使用了三个阶段训练，自学习阶段对没有的量化低精度学生网络进行微调，以获得良好的初始化。其次，共同学习阶段训练一名教师。最后，辅导阶段将知识从受过培训的教师转移给学生。但是</p>
<p>Model compression via distillation and quantization
Polino等人提出了两种新的压缩方法，联合利用权重量化和将较大的教师网络蒸馏为较小的学生网络。第一种方法称为量化蒸馏，在训练过程中利用蒸馏，将相对于教师表示的蒸馏损失合并到学生网络的训练中。第二种方法是可微量化，通过随机梯度下降优化量化点的位置，以更好地拟合教师模型的行为。量化的浅层学生可以达到与全精度教师模型相似的准确度水平。</p>
<p>PQK: Model Compression via Pruning, Quantization, and Knowledge
Distillation</p>
<p>kim等人提出了一种新颖的模型压缩方法，称为pqk，由剪枝、量化和知识蒸馏过程组成。与传统剪枝和蒸馏不同，pqk利用剪枝过程中剪枝的不重要权重来制作教师网络，用于训练更好的学生网络，而无需预先训练教师模型。
pqk有两个阶段。第一阶段利用迭代剪枝和量化感知训练来创建轻量级且节能的模型。在第二阶段，我们通过将第一阶段中未使用的不重要权重添加到修剪后的网络中来创建教师网络。通过使用这个教师网络，我们将修剪后的网络训练为学生网络。这样不需要为蒸馏框架预先训练教师网络，因为教师网络和学生网络共存于同一网络中。但是这样无法再原先未量化得工作上建立更好的结果。</p>
<h2 id="暗光成像国内外发展现状">暗光成像国内外发展现状</h2>
<p>由于一些不可避免的环境限制，如光线不足，曝光时间较短，光线不均匀等，在这种条件下拍摄的图片美感受损，同时这种图片对于后续的算法，如对象跟踪，识别检测之类的任务可能效果较差，使用暗光增强网络来处理图片是解决此类问题的一种手段，
传统的用于暗光增强的方法有直方图均衡化，基于Retinex方法，后者关注相对较多（暗光综述pdf，第一张的第二段）。基于Retinex模型的方法通过一些先验或者使用正则化将暗光图像分解为反射分量和照明分量，其中估计的反射分量是增减结果的参考，但是这种方案是有一定问题的，因为将反射分量视为增强结果的假设不一定成立，如果碰到一些比较特殊的光照，这种增强可能会导致不正常的结果，例如细节丢失或者颜色失真，同时由于暗光条件先图像的噪声会很大，而在增强过程中保留了并放大了噪声。而且Retinex优化时间较长，同时找到一个合适的分解图像的正则化或者先验比较困难，所以有一些局限性。</p>
<p>自从深度学习发展壮大，人们开始转向使用深度学习解决暗光增强问题，与传统方法相比，深度学习方法具有更好的速度、鲁棒性和准确性，于是越来越受到关注。自2017年以来，深度学习的解决方案数量逐年增加，主要使用的策略有监督学习、强化学习、无监督学习、零样本学习、半监督学习。</p>
<p>对于监督学习的暗光增强方法，可以分为端到端、基于深度Retinex以及实际数据驱动的方法，第一类方法中LLNet[1c]使用stacked-sparse降噪自动编解码器[56]去给暗光图像提亮以及降噪。这个开创性的工作启发了后人使用端到端网络去处理暗光增强问题,Lv等人[3]提出了端到端多分支增强网络（MBLLEN），MBLLEN提高了LLIE（Low-light
Image
Enhancement）的性能，其使用特征抽取模块、特征增强模块以及特征融合模块抽取有效的特征，Lv等人[15]之后再次做出改进，使用三个网络完成暗光增强，分别有光照网络，融合网络以及恢复网络来进一步提高性能。Ren等人[12]设计了更为复杂的网络，使用图像增强的编码器-解码器结构的网络以及图像边缘增强的递归神经网络。与Ren等人[12]类似，Zhu等人[16]提出了一(edge-enhanced
multi-exposure fusion
network)EEMEFN方法，EEMEF方法包括两个阶段：多曝光融合和边缘增强，其产生的两个分支可以获得两套增强的结果，最后采用简单的平均方案将两个增强之后的结果进行融合并通过一个细化单元对结果进行进一步的细化。一些人也在LLIE的结构上引入了金字塔网络（LPNet）[18]，残差网络[19]，拉普莱斯金字塔网络(DSLR)[21]，来优化性能。这些方案通过LLIE常用的端到端的结构高效的集成特征表示。对于暗光下他图像噪声的问题，Xu等人[57]对不同频率的噪声进行了不同对比度的观察，提出一种按照频率的大小来分解图片再增强图片的方法，在低频中恢复具有噪声的图像内容，并推理出高频的细节以完成暗光增强。
[23]提出了一种渐进式递归（progressive-recursive）暗光图像增强网络，其使用递归方法逐渐的增强输入图像。为了解决暗光下视频的不稳定，zhang等人提出学习单张图片的运动情况来强化视频在图像上的稳定性。Chen等人2018[learining
to see in
dark]年提出使用raw图像进行暗光成像网络的训练，来解决isp中间过程造成信息丢失的问题，并且提供了相应的数据集。Xing等人[Abandoning
the bayer-filter to see in
dark]使用彩色处理网络和单色处理网络结果融合并引入注意力模块将Chen等人的工作进一步的提生了效果。</p>
<p>由于有相关的可解释的物理基础，基于Retinex的深度学习方法也成为了一个流派，基于Retinex的方法通常将网络功能划分为计算照明图的网络和计算反射率的网络，使用两类分量进行暗光图像增强。[4]提出了retinex-net，网络将一个输入输出成两部分，一个是与光照无关的反射率图，另一个是照明图，和一个使用照明图进行暗光增强的网络。后来retinex-net通过添加新的一些限制和更加先进的网络设计，获得了更好的网络增强效果。为了减轻计算量，Li等人[5]提出了一种解决暗光增强的轻量级网络LightenNet，网络由四层组成，LightenNet将暗光图像作为输入，然后估计其照明图。将输入图像和照明图进行处理得到暗光增强之后的结果。Wang等人[60]提出了DeepUPE网络结构，DeepUPE提取全局和局部特征来学习图像照明的映射。Zhang等人[11]分别开发了三个网络，用于层分解，反射率调整，以及光照的调整，称之为KinD。此外作者通过照明的多尺度注意力模块减轻了KinD结果中存在的缺陷。为了解决基于Retinex的神经网络没有处理噪声的问题，Wang等人[10]提出了一种渐进式Retinex网络，使用两个网络，一个估计照明，另一个估计噪声，两个网络使用渐进的机制进行工作，直到能获取比较稳定的结果。Fan等人[14]结合了语义分割以及Retinex模型，提高了网络在真是场景下的增强新能，核心思想史使用语义信息的先验知识来知道增强时使用的照明分量和反射分量。</p>
<p>由于一些暗光数据的拍摄比较困难，所以一些方法使用了强化学习、无监督学习、零样本学习、半监督学习。Yu等人[25]提出使用强化学习来学习曝光照片，首先根据曝光情况将输入图像分为多个子图像。网络对每个子图像使用强化学习学习局部曝光，使用对抗性的学习来作为奖励评估函数，最后使用每个局部曝光的结果对输入进行合成，进而获得不同曝光下的合成图片，最终通过融合这些合成图片来实现暗光增强。在一些成对的数据集中进行训练可能会出现过拟合和泛化能力不强的问题，[26]提出了EnligthenGAN的无监督学习方法。器材作用注意力机制来引导Unet网络作为生成器，使用全局和局部的鉴别器来保证暗光增强之后的结果和正常的真实光照一致，其提出的全局和局部特征保持损失函数可以保证增强前后的图像内容不会有损失。零样本学习主要的使用不成对的数据进行训练，Zhang等人[27]提出了一种零样本学习方法ExCNet，其首先使用网络估计最适合于暗光图像的S形曲线，确定S曲线之后，使用一个引导滤波器将整个输入图像分成基本层和细节层，然后使用S曲线调整基本层，使用Weber对比度[65]来融合细节层和经过处理之后的基本层来得到暗光增强的图像。Zhu等人[29]提出了一种三分之CNN网络，叫做RRDNet，通过对输入图像分解为照度、噪声以及反射率来进行增强。为了结合无监督学习和监督学习的优势，有人提出了半监督学习的暗光增强方法，Yang等人[33]提出了一种半监督深度递归网络（DRBN）。DRBN首先使用监督学习完成增强图像的线性表示，然后通过无监督对抗学习来重新组合给定线性的线性表示来改进表示方法，并且DRBN引入长期记忆网络实现了更好的性能。</p>
<p>在众多暗光增强方法中监督学习目前还主要是主流方案，本文也采用监督学习的暗光增强方法进行量化研究。</p>
<h2 id="本文主要工作">本文主要工作</h2>
<h2 id="相关章节安排">相关章节安排</h2>
<h1 id="相关理论研究">相关理论研究</h1>
<p>量化与蒸馏方法是本工作的重点，量化框架会分别实现量化与蒸馏的相关算法，来辅助完成本工作。在这里介绍一下蒸馏与量化的相关理论</p>
<h2 id="量化">量化</h2>
<p>在过去的几十年里，神经网络通过增大参数量来实现了对复杂问题的有效解决，虽然效果有所提升，但是这些模型的参数量问题使得其不能很好的部署在资源首先得设备上，这为实现深度学习的普及带来了问题，此类学习应该在资源受限制的环境下以保证任务的正确性，这将对自动驾驶，医疗监控，安防等各个需要深度学习的领域带来不小的帮助。神经网络的步数已经有大量文献在此方向上做努力，主要可以分为三个方法：量化、剪枝、蒸馏，一般在嵌入式设备部署网络之前会选择使用量化方法，主要是量化是专门为硬件部署方案设计的，其次是剪枝虽然会去除不必要的权重层，但是这样做会有一定降低网络效果的可能性，蒸馏的话更需要工程师设计出更好的网络结果，如果设计不好依旧会导致网络不能达标，因为部署前最省力的办法是使用量化压缩模型。</p>
<p><a
href="Robert%20M.%20Gray%20and%20David%20L.%20Neuhoff.%20Quantization.%20IEEE%20transactions%20on%20information%20theory,44(6):2325–2383,%201998.">参考文献1</a></p>
<p>在数学中，量化使用数值近似的方案处理连续数学量问题，这个领域有很悠久的历史，随着计算机的出现，这个问题也一起了人们的一些兴趣，在数值分析中，有时为了解决一些数学问题，使用某些理想化的解法为参照来准确实现该计算机算法得到的结果并不理想，因为在算法运行的过程中存在舍入和截断误差，这些舍入和截断误差主要和用于计算的数据仅用有限个比特表示有关，例如IEEE规定的对数据的表示。为了解决这些算法实现问题，有相关的学科给出了一些答案。</p>
<p>毫无疑问，现在已有大量关于神经网络量化问题的论文，这些量化问题可以说是对以前的数学上的量化问题的扩展与重新认识，神经网络给量化带来了机遇和挑战。首先，目前大多数神经网络参数都比较大，极有可能造成参数过于冗余，因此有很大可能性出现在不影响最终网络效果的条件下降低精度，其次神经网络多数是矩阵运算，其推理与训练是计算密集型，数值的有效表示尤为重要，</p>
<p>量化主要目的在于减小计算量，缓解机器中的缓存消耗从而降低算法可部署机器算力的下限。主要原理是使用定点数运算代替浮点数，例如32位浮点数运算代替8位定点数去做运算，计算量会大大减少，推理速度也会提升，google曾做过统计，使用cpu或者dsp进行浮点数推理和定点数推理时间能相差2-3倍，如果使用向量处理器的话，因为其单指令多数据流的操作方案可以很轻松的实现向量操作，进而速度能提升近10倍。但是，虽然量化带来了诸多好处，却因为参与计算参数的位数降低导致整体计算结果变差，为了缓解这种变差的趋势，需要先了解量化的运算过程，才能了解量化应该在什么地方优化。</p>
<h3 id="量化的基本思路">量化的基本思路</h3>
<p>再众多量化方法中，量化的一些基本概念是共通的，本文也是使用如下的一些量化技巧来完成量化，下面简要介绍下量化相关的一些基本思路。</p>
<h4 id="量化的主流方法">量化的主流方法</h4>
<ol type="1">
<li>二值化</li>
</ol>
<p>二值化的方法简单粗暴，计算参数只有0、1，使用异或运算和移位运算来代替卷积中代价较高的乘法和加法，一般适用于arm平台。近年来，有许多二值化神经网络方法被提出，从开始的使用确定的函数对权重和输入量化的朴素方法，到从多角度优化量化参数的方法，如通过修改损失函数来来限制权重，通过修改网络结构减小信息量损失，通过减小梯度误差进行训练，最小化量化误差。但是由于模型二值化之后模型精简太多，对于暗光增强等图像增强这种需要更多信息量的任务效果并不是很好。二值化的朴素做法：</p>
<p><span class="math display">\[
Q(x) = sgn(x) = +1 (x &gt;= 0), -1 (x &lt; 0)
\]</span></p>
<ol start="2" type="1">
<li>线性量化</li>
</ol>
<p>一种较为常见的方法，通过确定零点和映射尺度来进行量化计算。在arm，x86，英伟达gpu等架构的芯片上都支持8位定点数运算，由于线性量化的计算方式的问题，其量化与反量化的过程都是向量操作，使用向量处理器能更好的提升性能。虽然现在低位数的量化方案层出不穷，但是限于硬件，低位运算的开发板需要更多的开发周期，一般工业上部署主要是使用8位以上的量化。</p>
<ol start="3" type="1">
<li>对数量化</li>
</ol>
<p>对数量化是通过位移运算来加速推理的，将每一个参数视为一个以2为底的指数，在实际推理过程中指数的运算可以看做是移位运算，
<a
target="_blank" rel="noopener" href="https://blog.csdn.net/qq_20759449/article/details/104733457">对数量化相关文章</a>
<a
target="_blank" rel="noopener" href="https://pic2.zhimg.com/80/v2-a4f481df63477b7af50dcf571d3121ad_720w.webp">对数量化</a></p>
<h4 id="线性量化的数学原理">线性量化的数学原理</h4>
<p>由于本文使用线性量化方案，所以在这里主要介绍线性量化。线性量化主要有两组参数，一组用于控制映射后参数变化范围，叫做scale，另一组用于映射之后数据的原0点映射完之后变为什么值，这个变量叫做<span
class="math inline">\(zero\_point\)</span>
于是两户使用如下公式即可完成： <span class="math display">\[
    x_{ori} = x_{scale} \times (x_{quant} - x_{zero\_point})
    x_{quant} = round(x_{oir} / x_{scale}) + x_{zero\_point}
\]</span></p>
<figure>
<img src="参数解释.png" alt="数解释" />
<figcaption aria-hidden="true">数解释</figcaption>
</figure>
<figure>
<img src="float_to_int_map.png" alt="浮点数映射成定点数" />
<figcaption aria-hidden="true">浮点数映射成定点数</figcaption>
</figure>
<p>现在这只是进行了映射，映射后的结果进行计算之后应该如何参与运算呢，这里以两个浮点数做乘法为例讲解整个过程
<img src="quant_calculate_method.png" alt="quant_calculate_method" />
可以看到，除了几个scale参数外其他参数都是定点数，浮点数可以提前计算出来，放在模型里，这样在模型做推倒的时候可以很方便的拿来使用。</p>
<p>在实际生产环境中，可能一些定点数运算可能会超出8位整数，所以运算中的寄存器位数会比8位要多，防止出现溢出。同时我们将映射之后的位置使用<span
class="math inline">\(zero\_point\)</span>标定是有意义的，因为网络中有可能会进行0填充等涉及0的一些常见操作，这样的设计会使得，量化方案损失更小。</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/505570612">参考资料</a></p>
<h4 id="量化的一些细节">量化的一些细节</h4>
<h5
id="逐层量化逐通道量化和分组量化">逐层量化、逐通道量化和分组量化</h5>
<p>在大多数计算机视觉任务中，层的激活输入和许多不同的卷积滤波器进行卷积，由于卷积中卷积层因为通道数量有很多，每个通道对应一个卷积核，卷积计算时需要每一个卷积核的计算相互独立，同时每一个卷积核的数据分布是不同的，所以量化时，对于卷积等内部有计算相互独立的参数有逐层和逐通道量化。</p>
<ol type="1">
<li>逐层量化</li>
</ol>
<p>将神经网络中的某一层中参数视为一个整体，通过相应的统计方法计算出一个量化相关的参数，例如层中最大值，最小值等，但是这个参数是关于神经网络整个一层的。这种方法看起来简单但是可能会导致次优的精度，因为两个卷积核的分布可能不一致。</p>
<ol start="2" type="1">
<li>逐通道量化</li>
</ol>
<p>将神经网络中的独立运算参数，利用相应的统计方法计算出一个量化相关的参数，但是一层里面会有一组参数，分别对应各个通道的量化方案，这样呢嗯呢该使得量化的粒度更细。其优点在于因为量化区间更加灵活，有助于在不同卷积核之间的参数分布变化很大的情况下使用，对网络精度会有一定提升，但缺点就是需要额外的量化参数存储开销</p>
<ol start="3" type="1">
<li>分组量化</li>
</ol>
<p>逐通道量化和住蹭了量化各有利弊，为了中和其中的利弊出现了逐通道量化，将多个通道分组，来计算裁剪范围，这种量化方法在有全连接的注意力层组成的transformer模型中是有用的。</p>
<p>如果不计存储成本的话，那么逐通道方法是推荐使用的，本文在量化感知训练中，卷积层的量化就使用了逐通道量化。</p>
<h5
id="对称量化非对称量化和随机量化">对称量化、非对称量化和随机量化</h5>
<ol type="1">
<li>对称量化</li>
</ol>
<p>对称量化比较简单，直接将<span
class="math inline">\(zero\_point\)</span>指定为0，之后观察计算表达式，可以发现
<img src="对称量化计算量.png" alt="对称量化计算量" />
这样很方便使用向量处理器做矩阵运算。
表达式中少了两次运算。但是会带来一些其他的问题，例如relu激活函数的到的值都是非负值，这个时候量化需要考虑使用有符号量化还是无符号量化</p>
<ol start="2" type="1">
<li>非对称量化</li>
</ol>
<p>非对称量化就是刚刚提到的<span
class="math inline">\(zero\_point\)</span>不指定为0的情况，这种量化方案更加灵活，但是会带来一些额外的储存开销，模型中必须保存零点运算相关参数，同时分析量化计算表达式也可以知道，如果<span
class="math inline">\(zero\_point \not =
0\)</span>将会导致一些额外的计算开销。</p>
<p>当然两种量化方案可以混合使用，可以对weight使用对称量化，而对activation使用非对称量化以达到性能的均衡。</p>
<ol start="3" type="1">
<li>随机量化</li>
</ol>
<p>随机量化将量化器建模为加性噪声，然后进行舍入。随机量化器由下式给出</p>
<figure>
<img src="suiji_quant.png" alt="suiji_quant" />
<figcaption aria-hidden="true">suiji_quant</figcaption>
</figure>
<p>可以看到在量化的过程中加入了噪声，加入噪声的原因是实际推理中输入是随机的，所以为了得到鲁邦性能较强的浮点数权重需要引入随机误差，这导致了权重会随着batch的不停而发生变化，在训练的开始随机量化效果会很差，但是在后续的训练过程中会弥补这一问题，但是由于量化过程中引入了随机数，一般的硬件不支持这样的推理，所以这种方法并不实用</p>
<h5 id="混合精度量化和单精度量化">混合精度量化和单精度量化</h5>
<p>量化可以根据精度使用情况来分类，量化过程中如果一些位数使用是多种情况的，比如3,4,5位等情况，这时的量化时混合精度量化，如果量化位数是固定的就是单精度量化，当前学术界对于混合精度量化研究比较多，但是工业上使用较多的是单精度量化，原因是单精度量化在开发板上比较容易实现，工程开发和硬件实现难度较低，但是混合精度量化需要处理器同时支持多种位数的整数计算，在硬件设计上难度较大，并且在代码设计上需要不断的考虑数据类型转换，会增加工程的复杂性。</p>
<h4 id="量化训练的方法">量化训练的方法</h4>
<p>前面我们说了一些量化的分类，在实际部署模型中，理论方法确定是一方面，另一方面是执行方案。量化推理已经有充足得理论支撑，但是量化中关于某个模型得一些具体量化参数应该怎样确定是一个需要解决的问题，本文就是来解决这样的问题的，现在有多种多样的算法，其计算推理方式都如上所示，但是不同方法的不同之处在于如何确定好scale和zero_point。</p>
<h5 id="量化后训练和训练后量化">量化后训练和训练后量化</h5>
<p>量化后训练和训练后量化如其名字一般，一个是先训练然后在量化，一个是先量化然后再训练。</p>
<p>训练后量化是一种通过32位浮点数网络经过训练得到比较好的结果之后,然后对得到的模型进行量化参数确定，好处就在于，对于一个模型不需要他人的训练代码，不需要他人数据集和标签，只需要模型就能进行量化，对于一些涉密等未公开的工作可以使用。训练后量化的一种简单实现就是直接降低权重的精度到8位，因为质量化权重，不涉及输入参数的量化，所以这种方法不需要数据集，操作十分简单。当然这种方法的缺点就是该方法不能将量化模型训练的很好，正确率较低，对于一些低bit量化很有可能达不到理想的效果，主要是因为在量化的过程中引入的误差没有在训练的时候考虑到。</p>
<p>这中训练后量化由于会发生效果较差的问题，所以有一些调优的办法,
以下过程能帮助我们了解模型效果差通常是出现在哪些地方。</p>
<ol type="1">
<li>推理流程结果检查：</li>
</ol>
<p>一个重要的部分就是保证32浮点数模型和量化后模型在推理结果能比较一致，将量化模型设置为32位进行推理和原模型进行对比，必要的时候可以查看两者层间结果的匹配程度。</p>
<ol start="2" type="1">
<li>位数变化检查：</li>
</ol>
<p>通过对激活值以及权重值位数的影响来判断，位数的变化对于性能提升是否有很大影响，如果有比较大的影响，说明性能瓶颈就在位数上。</p>
<ol start="3" type="1">
<li>逐层分析</li>
</ol>
<p>将某一层量化保留，其余层还原为32位，查看量化之后的效果，通过这种单独查看某一层量化效果来观察是否所得结果是可接受的，以此来判断某一层量化是否合理。</p>
<ol start="4" type="1">
<li>可视化分析</li>
</ol>
<p>如果确定是某一层的问题，也可以使用张量分析，查看权重和激活值的张量分布，进一步确定问题所在的地方。</p>
<figure>
<img src="a量化训练流程.png" alt="a量化训练流程" />
<figcaption aria-hidden="true">a量化训练流程</figcaption>
</figure>
<p>针对训练后量化的问题，有人提出了量化感知训练，量化感知训练是一种通过在训练神经网络过程中引入模型量化误差来寻找最优解的一种方法，这种方式训练出的量化方案更为灵活，量化前还需要计算激活值的数据分布，从而计算出激活值的量化参数，一般来说100batch就能很好的统计出激活值的数据分布。</p>
<p>google对于一些量化基础方法进行了实验，在量化感知训练中，逐通道的量化是效果最好的，因为权重的量化精度损失会比较小，同时逐层量化效果性能较差，激活值的8bit量化精度损失比较小，参数较多的网络量化损失比较小.</p>
<p>总的来说，量化感知训练会使网络输出准确性有所提升，但是其一大缺点就是实现起来不方便，由于需要模拟量化计算，量化感知训练需要手动实现一些网络层，然后再训练过程中进行替换，并且需要进行一些训练逻辑的确定，对于一个网络的量化需要很多工作量，本文使用了pytorch.fx技术可以很好的避免这一情况。</p>
<h5 id="量化数据的统计">量化数据的统计</h5>
<p>网络中数据按照每次模型推理输入数据的不同可以分为两种，一种是对于每次输入都不变的权重weight，另一种是在网络中随输入改变而改变的activation，activation实际上是网络中各个层计算出来的结果，在进行量化推理的时候两种参数都需要进行量化和反量化，考虑到weight在部署时参数会固定，所以weight量化参数很好确定，一般使用weight的最大值和最小值表示量化的区间即可，但是由于网络中的activation会根据输入的不同而不同，对于activation的量化可以采用统计等方案进行，统计中也有一些算法，比如统计activation的分布，然后去除前后3%的参数，剩余的部分为量化的范围
<img src="activation.png" alt="activation_calculate" />
当然还有一些其他的统计方案，主要还是为了能更好的将模型中的参数在量化之后能均匀的分布在[-128,127]之间。</p>
<h5 id="如何进行梯度反向传播">如何进行梯度反向传播</h5>
<p>观察量化表达式会发现，从浮点数到定点数会有一个取整的过程，这个round取整函数实际上是没有办法进行求导的，这在训练后量化的模型中没有问题，但是在量化后训练的方式中会有求导的问题，因为神经网络是一种依靠求导得到梯度，在通过将参数向梯度方向移动求得最小值的一种方法，因为round函数的存在，则必须要寻找一种方案去代替求导的方案，一种方案是使用STE(Straight-through
estimator)的方法，他的思路是将round的导数视为1，这可以在反向传播部分将他写上，方法简单直接，效果也非常好，以至于一直以来量化领域一直使用这种方法进行量化后训练的反向传播，</p>
<p><a
target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/570322025">请在这里分析STE的数学原理</a></p>
<p><img
src="https://pic1.zhimg.com/v2-c3b5f595b8680eb04e609ec2b85e9bec_r.jpg"
alt="量化后反向传播" />; <img
src="https://pic1.zhimg.com/80/v2-a74fade2b52cb69d8d4323aef05db3bc_720w.jpg"
alt="量化后反向传播" />
当前也有人对其进行过改进，就是使用一些特殊的函数去模拟round函数，例如sigmod其与取整函数很相似，经过叠加之后就可以达到round函数的效果，DSQ论文只能怪就是使用了这样的方法，也有人严谨分析了STE的做法提出了多级量化反向传播的方案</p>
<h5 id="量化训练的主要流程">量化训练的主要流程</h5>
<p>在量化推理中，为了达到最好的训练效果，除了有良好的量化方法，还需要再训练上使用最佳的方法，如果训练的步骤不正确则会导致整个方法变差，经过多次量化实践可以总结出的训练步骤有如下几条：</p>
<ol type="1">
<li>量化后的模型不要从头开始训练，因为量化感知训练需要进行激活值分布的统计，如果从头开始最开始的激活值的分布是不满足后续训练好的模型，最终模型会进入一个比较差的状态。所以最佳的做法应该是一开始加载原模型，或者在模型加一个开关，控制模型是否进行量化训练，将未量化状态的模型训练到最优，此时打开量化开关进行量化感知训练。</li>
<li>统计激活值分布，确定激活值的量化范围是什么，有助于后续量化参数的确定。激活值分布统计次数不宜过多，一次足以，多次的统计会让模型难以收敛，因为统计后的参数会让模型中每一层的输入发生变化，造成模型又一次重新训练。</li>
<li>可以使用batchnormal和relu6等能规范化输入的层使得层与层之间的参数相互独立，每一层的激活值分布较为固定，这样激活值统计情况比较准确，但是batchnormal对于卷积操作或者全连接操作过程中的逐层量化方法影响比较大，因为batchnormal层学习了偏移和缩放的参数，会使得通道与通道之间的分布差异越来越大，使得逐层量化不再使用</li>
<li>网络设计时参数越多其鲁棒性越好，因为参数越多，冗余越多，越能抵挡的住量化带来的精度损失。</li>
</ol>
<figure>
<img src="" alt="量化训练流程图" />
<figcaption aria-hidden="true">量化训练流程图</figcaption>
</figure>
<h2 id="蒸馏">蒸馏</h2>
<p><a
href="小绿鲸中的“知识蒸馏综述”">知识蒸馏综述，相关论文请在原文查找</a></p>
<p>深度学习是人工智能成功的基础，大规模深度模型已经取得了压倒性的成功，但巨大的计算复杂性和巨大的存储需求使其在实时应用中部署成为一个巨大的挑战，知识蒸馏主要解决的问题和量化相同，作为一种代表性的模型压缩和加速方法，知识蒸馏可以从较大的深度神经网络提取到较小的网络中，，得到了越来越多的关注，由于本文也是用了蒸馏方法，这里简要介绍一下蒸馏的相关内容。</p>
<h3 id="蒸馏的发展">蒸馏的发展</h3>
<p>Bucilua等人（2006）首先提出从大模型或模型集合转移到训练小模型中，同时网络输出精度并没有所下降。Urner等人，2011提出在半监督学习中，使用未标记数据和教师网络的输出来监督学生网络来进行学习，从这之后，从大模型中向小模型学习就叫做了知识蒸馏。知识蒸馏主要的步骤是学生网络模仿教师网络的输出以及中间的一些结果，从而获得学习到教师网络中的一些特征等一些东西，产生更好的效果。有了这个转移的思路，后面单位问题就主要是如何进行知识的转移，知识蒸馏主要分为三个部分：知识、师生架构，提炼算法。</p>
<figure>
<img src="knowledge_disstrill.png" alt="knowledge_disstrill_框架" />
<figcaption aria-hidden="true">knowledge_disstrill_框架</figcaption>
</figure>
<p>尽管知识蒸馏很有效，但是关于知识蒸馏的书籍并不是很多，Urner等人（2011）证明了使用没有标签的数据集和教师模型与学生网络进行知识蒸馏的过程是PAC可学习的，为了理解知识蒸馏的工作原理，（Phuong和Lampert）（2019）给出了一个知识蒸馏的理论依据，这个理论给出了知识蒸馏中学生网络学习的内容和学生网络学习的速度，并阐明知识蒸馏成功的原因以及必要条件，一个成功的知识蒸馏需要依赖于数据集，学生网络学习的目标，以及用于学习的损失函数。
<a
href="Cheng,%20X.,%20Rao,%20Z.,%20Chen,%20Y.,%20&amp;%20Zhang,%20Q.%202020.Explaining%20Knowledge%20Distillation%20by%20Quantifying%20the%20Knowledge.%20In:%20CVPR.">Cheng</a>
等人从深度神经网络的中间层的可视特征来解释知识蒸馏， <a
href="Ji,%20G.,%20&amp;%20Zhu,%20Z.%202020.%20Knowledge%20Distillation%20inWide%20Neural%20Networks:%20Risk%20Bound,%20Data%20Efficiency%20and%20Imperfect%20Teacher.%20In:%20NeurIPS.">Ji和Zhu</a>的理论从数据效率教师网络不完善等方面解释了知识蒸馏在神经网络中的作用。[Cho和Hariharan]对知识蒸馏中大模型的作用进行了详尽的证明，其表明较大的模型未必就是更好的老师，可能会对学生网络的学习造成不利的影响，Tang
等认为知识蒸馏可以用来做教师网络的准确性评估以及最佳模型网络结构的探索。</p>
<p>受到人类的学习模式的启发，知识蒸馏出现了多种多样的方式，teacher-student
learning（Hinton et al.，2015）、mutual learning（Zhang et
al.，2018b）、assistant teaching （Mirzadeh et al.，2020）、lifelong
learning（Zhai et al.，2019）和self learning（Yuan et
al.，2017），知识蒸馏的大多数扩展都集中在深度学习网络上，由此产生产生的轻量网络可以部署在边缘设备上。受到模型压缩和知识提取的启发，数据集提取也出现了，它用于将知识从大数据集转移到小数据集，从而减轻模型训练时的载荷。
（pdf第三页）</p>
<h3 id="蒸馏的基本概念">蒸馏的基本概念</h3>
<p>知识蒸馏中，</p>
<h4 id="蒸馏的种类">蒸馏的种类</h4>
<p>根据蒸馏的特征不同可以将蒸馏分为如下：</p>
<ol type="1">
<li>基于响应的知识蒸馏
这种方式主要是使用教师的最终输出作为学生网络的学习对象，基于相应的知识蒸馏简单有效，其损失函数如下：
<span class="math display">\[
L_{ResD}(Z_t,Z_s) = L_R(Z_t,Z_s)
\]</span> <span class="math inline">\(L_R\)</span>是logits的KL散度,<span
class="math inline">\(Z_t,Z_s\)</span>是网络的logits，即全连接层输出的未归一化的概率，主要目的就是让学生网络及尽量贴合教师网络，方法简单有效，(Hinton
et al.,2015; Ba and Caruana,
2014)等人使用温度参数实现了对软目标的学习，在图像分类中比较流行，其将原来的<span
class="math inline">\(Z_t\)</span>变为如下向量：</li>
</ol>
<p><span class="math display">\[
p(z_i,T) = \frac{exp(z_i / T)}{\sum_{j}exp(z_j/T)}
\]</span> 其中T为温度参数，用于控制蒸馏目标的重要性，于是损失函数变为：
<span class="math display">\[
L_{ResD}(p(z_t, T), p(z_s, T)) = L_R(p(z_t, T), p(z_s, T)) .
\]</span></p>
<p>然而基于反应的知识蒸馏比较依赖于最后一层的网络输出，无法获得教师网络中的中间输出，这对于网络学习神经网络深层信息没有帮助，由于logits只是概率分布，所以基于响应的知识蒸馏常用在监督学习上</p>
<ol start="2" type="1">
<li>基于特征的知识蒸馏</li>
</ol>
<p>深度神经网络因为足够的深，擅长学习更为抽象的信息，从而形成更为抽象的特征，因此最后一层和中间层的输出，即特征图，都可以作为学生模型学习的对象。特征图是对知识基于相应的更好的扩展，基于特征的知识蒸馏就是对神经网络更深层次的特征进行学习的一种方案。Romero等人，2015在fitnet中引入基于特征的知识蒸馏，主要做法是匹配教师和学生的特征激活，后续(Zagoruyko和Komodakis，2017；Kim等人，2018；Heo等人，2019c；Passban等人，2021；Chen等人，2021年；Wang等人，2020b）参照此法提出了多种特征蒸馏方案。Huang和Wang(2017)使用神经元选择性转移，Passails和Tefas通过对特征空间概率分布进行学习来实现知识蒸馏，Kim等人使用一种叫做factor一种中间形式来缩小教师和学生网络了之间的差距，Chen等人提出了夸层知识学习，通过注意力机制为每个学生网络层分配一个合适的教师层去学习，等等。</p>
<figure>
<img src="feature_diss_method.png" alt="feature_diss_method_table" />
<figcaption aria-hidden="true">feature_diss_method_table</figcaption>
</figure>
<p>通常来讲，特征蒸馏的损失函数能被公式化为 <span
class="math display">\[
L_{FeaD}(f_t(x),f_s(x)) = L_F(\phi_{t}(f_{t}(x)), \phi_{s}(f_s(x)))
\]</span> 其中<span class="math inline">\(f_t{x}\)</span>和<span
class="math inline">\(f_s(x)\)</span>表示的是教师网络和学生网络中间层的特征图，同时<span
class="math inline">\(\phi(f_t(x))\)</span>和<span
class="math inline">\(\phi(f_s{x})\)</span>表示的是教师网络和学生网络中特征图的映射，因为教师网络和学生网络结构不一致，所以有可能会出现学生网络学习教师网络的特征向量尺度不一致，需要一个函数来进行特征的变形，<span
class="math inline">\(L_F\)</span>表示特征相似性函数，这个函数应该在特征越相似的情况下大小越小，使得能够很好的进行优化。特征蒸馏的损失函数具体来说有<span
class="math inline">\(L_2(.)、L_1(.)、L_{CE}(.)\)</span>以及<span
class="math inline">\(L_{MMD}(.)\)</span>，分别是L2范数距离损失，L1范数距离损失，交叉熵损失函数和最大平均差异损失。尽管特征知识能为学生网络提供有用的知识，但是选择哪些层进行特征蒸馏是一个有待讨论的问题，本文提出了一种使用激活值hessian矩阵的方法来判断特征的重要性，使用数学推导来判断修改某一层参数发生改变之后会对整个结果造成怎样的影响，依次来决定蒸馏的对象，后文将进行展开描述。</p>
<figure>
<img src="feature蒸馏流程.png" alt="feature蒸馏流程" />
<figcaption aria-hidden="true">feature蒸馏流程</figcaption>
</figure>
<ol start="3" type="1">
<li>基于关系的知识蒸馏</li>
</ol>
<p>基于反应和基于特征的蒸馏方式都是用教师模型中特定的层作为学习对象，这仅仅学习了单独层的特征信息，却忽视了整个网络中其它层之间的关系信息，基于关系的知识蒸馏继续深入探索了网络中权重和权重之间以及权重和数据样本之间的关系，一般就特征图关系的关系知识蒸馏损失函数可以公式化为：
<span class="math display">\[
L_{RelD}(f_{t},f_{s}) = L_{R^{1}}(\phi(f_{t1},f_{t2}),\phi(f_{s1},
f_{s2}))
\]</span> <span class="math inline">\(f_t\)</span>和<span
class="math inline">\(f_s\)</span>分别是教师网络和学生网络的特征图对,<span
class="math inline">\(f_{t1},f_{t2}\)</span>是教师网络中选中的特征对，相应的<span
class="math inline">\(f_{s1},f_{s2}\)</span>是学生网络中选出的特征对，<span
class="math inline">\(\phi()\)</span>函数表示的是特征对相似性函数<span
class="math inline">\(L_{R^{1}}\)</span>表示蒸馏过程中的特征损失函数，关系之间的相似学习就是通过这个损失函数来实现的，</p>
<figure>
<img src="relationship_distillation.png"
alt="relationship_distillation" />
<figcaption aria-hidden="true">relationship_distillation</figcaption>
</figure>
<h4 id="蒸馏的方案">蒸馏的方案</h4>
<ol type="1">
<li>离线蒸馏</li>
</ol>
<p>以前大多数蒸馏方案都是离线的，蒸馏预训练好的模型转移到学生模型，整个训练过程分为两步，首先使用一组训练样本在大的教师模型上进行训练，然后将训练好的教师模型提取logits或者网络的中间特征，将提取的结果用于学生网络的训练。第一步和往常训练一样，假定教师网络是与训练好的模型，所以离线方法的注意力交点主要在学生网络的结构上，以及如何定义教师网络的知识。离线方法的主要优点是简单且易于实现。教师网络可能在其他机器上训练，而用于提取知识的网络在其他机器上训练，这极大的方便了蒸馏的实施。离线蒸馏方法采用的是单方向的知识转移和两阶段训练，所以教师网络大模型的复杂的训练过程是不可避免的，同时学生网络和教师网路之间的性能差距始终存在，学生的性能很大程度上依赖与教师网络的性能。</p>
<ol start="2" type="1">
<li>在线蒸馏</li>
</ol>
<p>尽管离线蒸馏方案实施简单有效，但是离线蒸馏也有一些问题，为了克服离线蒸馏的限制，有人提出了在线蒸馏方案以进一步的提高学生网络的性能，尤其是在教师网络参数量不大的情况下。在线蒸馏中，教师网络没有进行预训练，但是教师网络会和学生网络一同进行训练，虽然在线蒸馏是一种具有高效并行的端到端的训练方案，在线训练方案通常无法容忍较大的教师模型，本文也采用了在线蒸馏方案，但是本文的蒸馏方案很好的避开了教师网络模型交到的问题。</p>
<ol start="3" type="1">
<li>自蒸馏</li>
</ol>
<p>子蒸馏中，教师网络和学生网络使用的是同一个网络，这可以视为在线蒸馏的一种特例，他将网络中较深部分的知识提炼成较浅部分的知识，自蒸馏就像人类在自学知识一样，不需要其他的教师网络模型。</p>
<p>离线蒸馏、在线蒸馏、自蒸馏可以结合起来使用，可以进行优势互补。</p>
<h2 id="量化与蒸馏">量化与蒸馏</h2>
<p>量化与蒸馏</p>
<p>Yoojin Choi, Jihwan Choi, Mostafa El-Khamy, Jungwon Lee; Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR) Workshops, 2020, pp. 710-711
Yoojin等人提出了无数据的对抗性知识蒸馏，它最小化了来自生成器的任何对抗性样本的教师和（量化）学生的输出之间的最大距离。但是主要工作还是在目标检测网络中</p>
<p>QKD: Quantization-aware Knowledge Distillation
Kim等人提出了量化与知识蒸馏相结合来解决模型压缩问题，使用了三个阶段训练，自学习阶段对没有的量化低精度学生网络进行微调，以获得良好的初始化。其次，共同学习阶段训练一名教师。最后，辅导阶段将知识从受过培训的教师转移给学生。但是</p>
<p>Model compression via distillation and quantization
Polino等人提出了两种新的压缩方法，联合利用权重量化和将较大的教师网络蒸馏为较小的学生网络。第一种方法称为量化蒸馏，在训练过程中利用蒸馏，将相对于教师表示的蒸馏损失合并到学生网络的训练中。第二种方法是可微量化，通过随机梯度下降优化量化点的位置，以更好地拟合教师模型的行为。量化的浅层学生可以达到与全精度教师模型相似的准确度水平。</p>
<p>PQK: Model Compression via Pruning, Quantization, and Knowledge
Distillation</p>
<p>kim等人提出了一种新颖的模型压缩方法，称为pqk，由剪枝、量化和知识蒸馏过程组成。与传统剪枝和蒸馏不同，pqk利用剪枝过程中剪枝的不重要权重来制作教师网络，用于训练更好的学生网络，而无需预先训练教师模型。
pqk有两个阶段。第一阶段利用迭代剪枝和量化感知训练来创建轻量级且节能的模型。在第二阶段，我们通过将第一阶段中未使用的不重要权重添加到修剪后的网络中来创建教师网络。通过使用这个教师网络，我们将修剪后的网络训练为学生网络。这样不需要为蒸馏框架预先训练教师网络，因为教师网络和学生网络共存于同一网络中。但是这样无法再原先未量化得工作上建立更好的结果。</p>
<h2 id="图像评价指标psnr">图像评价指标PSNR</h2>
<p>(Peak Signal-to-Noise Ratio,
PSNR)通常被用作图像评价指标，因为其提供了一个关于参考图像和对比图像之间相似性的衡量标准，提供一个图像发生变化程度的衡量指标，以评估图像的质量。</p>
<h1 id="量化框架开发">量化框架开发</h1>
<p>一般的神经网络框架主要目的是提供一种量化部署前的训练问题的，在没有量化框架前，手动实现一个量化方法需要考虑一些问题，首先STE反向传播过程由于需要增加，并且计算方式需要以整数量化的方式进行，例如卷积，卷积在pytorch中使用函数将底层计算严格的封装，这些计算方式不能由神经网络计算库中现有的函数提供，所以这就需要我们重新从神经网络库中重新将原来已经存在得模块改写，完成我们自身需要的量化逻辑，这本身是一个很大的工程，并且经过改动之后训练代码和量化框架会高度耦合，不方便再次利用。经过技术的革新，一些网络库也逐渐支持量化相关操作，但是这些量化组件需要很深刻的理解内部运行才能完成良好的量化工作，虽然现阶段借助于上述技术已经产生了很多框架，但是这些框架仅仅只支持量化，却不支持蒸馏，本工作参考了商汤以及华为的一些量化框架的实现，其中一个重点就是将量化蒸馏方案写成一个框架形式，方便各种类型的网络进行使用，在这里也是用了很多的pytorch中的技术，如torch.fx,torch.qat等等一些主要值得一提的技术</p>
<h2 id="torch.fx简介">torch.fx简介</h2>
<p><a
target="_blank" rel="noopener" href="http://giantpandacv.com/project/PyTorch/%E7%94%A8%E6%B2%90%E7%A5%9E%E7%9A%84%E6%96%B9%E6%B3%95%E9%98%85%E8%AF%BBPyTorch%20FX%E8%AE%BA%E6%96%87/">首先对fx进行介绍</a>
最开始pytorch训练框架本身不像tensorflow那样支持计算图的抽取，所以pytorch很难在网络代码固定之后再通过一些条件来修改网络结构实现动态图计算的功能，在一些真实场景中用户还是需要使用动态图来改变网络结构来进行神经网络性能方面的调整，在pytorch.fx出现之后，pytorch可以对网络计算图进行捕获和转换，实现了动态图计算。</p>
<p>神经网络框架分为两种，分别是静态图框架和动态图框架，Pytorch、Caffe、TensorFlow之间的不同点就是这几种方法的计算图是不一样的，pytorch每次计算使用新的图，TensorFlow进行主要使用静态图进行计算，所以TensorFlow中在运行模型前就将计算图的情况确定好了,两者的区别在于，静态图需要先构建在执行，好处在于可以将一些操作进行优化，例如，合并相关的算子，将常数折叠到其他网络中去。当前缺点也是有的，就是只有在网络运行之后才能看到相关的变量值。动态图是运算和构建同时进行的图，其好处在于网络搭建的过程中能够看到相关的变量值，可以方便的检查网络结构等正确性，但是缺点也有，由于是计算图是动态构建的，不知道一个操作后面的操作是什么，这就造成了后续优化很困难。
在pytorch.fx之前的一些静态图框架，如Caffe，还有TensorFlow，其设计了一个用于表示图的数据结构，只要用户调用相应的API就能实现对IR的修改和构建，之后得到这个计算图之后就可以进行各种需要的操作，如：并行加速、量化、性能优化等等，但是此类工具用起来比较难，并且一些开发环境比较麻烦，比如说需要使OneFlow实现上述功能，那么最终都是需要使用c++进行开发，同时在调试的过程中还需要掌握gdb以及pdb的使用命令，难以进行操作。到如今动态图已经发展起来，动态图由于其自身原因不能进行预测性结构改动的任务，这在静态图中很容易，动态图很难，Pytorch等使用动态图的框架急需改变这种处境，使得其可以使用户可以从用户的图程序中捕获出这种图结构来进行结构替换，以完成量化或算子融合。于是在之后Pytorch的发展中引入了TorchScript，其基于python程序中的AST来对网络IR进行构造，对整成程序进行整体建模，但是这样的方法会带来一些问题，就是这类操作的工程量太大，无论从技术方面还是从时间方面，并且在这样一个变换之中进行性能优化，量化结构替换，层融合操作将会更加困难。如果要改进这套方案的话需要简化一些操作，将原本对整个程序构建IR的方法改为对网络构建DAG（有向无换图）结构，然后提供一些对这个DAG图中的节点结构做替换的API来完成算子结构的修改，而不需要程序中隐藏的更高层的api结构，例如对卷积或者批标准化层操作的API。</p>
<p>Pytorch.fx实际上就满足上述所说的用户的需求，其能抽取神经网络计算相关的DAG图。通过官方给出的简单易用的修改计算图的接口，pytorch.fx就能很方便实现深度学习中的相关变换需求。这里总结一下其相关的核心功能：
1. 对神将网络的计算图捕获以及计算图的转换 2.
将神将网络中的计算简单的分为6种IR，用于表示捕获的计算节点中计算类型 3.
将变换后的计算图重新生成代码嵌入到相关位置</p>
<h2 id="torch.fx的相关原理及使用">torch.fx的相关原理及使用</h2>
<h3 id="torch.fx如何构建计算图">torch.fx如何构建计算图</h3>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/644590863">参考自</a></p>
<p>如果表达式的计算过程比较简单，那么对于计算图的绘制就会很容易，反之会很困难，但是在神经网络计算中，各种各样的计算都会出现，而且Pytorch是通过动态图进行计算的，所以中间出现的计算会更复杂，如条件语句，张量变形，一些深度学习中常用的一些包如numpy中的计算以及python和其他语言进行进行联合计算，如python调用c++中的相关算子，这些操作都会导致计算图的生成困难。pytorch.fx中基于历史原因给出了三种计算图的捕获方案，每一种方案能力都在变强，pytorch.fx最开始的版本中解决方案是使用符号追踪，符号追踪是一种将模型进行计算的过程，他使用Proxy作为伪输入，默认假设所有函数的参数都是Tensor，其不需要真正的运行，只需要相关的函数信息，就能模拟一次计算，在计算过程中Proxy会记录计算中执行的各种操作，这个方法的好处在于不需要进行实际的计算，缺点也是有的，就是张量的大小是没法知道的，如果有一次张量的切片或者其他的计算会导致运行出错的问题，更具体的说就是没有办法对变量的具体形式（如shape，dtype，requires_grad）做追踪。之后的pytorch.fx在升级的过程中修改了这一问题，在生成计算图时，除了需要一个Proxy之外，还需要一个torch张量用来模拟真实的运算情况，这种追踪计算图的方式叫做即时追踪，这样就解决了shape等参数无法得到的问题，即时追踪一般是指在运行时使用真实的输入数据进行追中，并且在追踪过程中计算预定义算子来实现计算图。及时追中仍然有一些问题，因为不同的输入，网络表现可能是不一样的，但是在构建计算图的时候却给出了一个特殊的输入，导致一些条件分支内的计算没有表现在计算图中，比如dropout和batchnormal层的操作在训练和推理时的运行计算过程是不一样的。基于这个问题pytorch.fx给出了最终的解决方案，使用动态优化，主要思路是将所有的函数调用进行统计，通过对字节码分析从中获取计算图以及相应计算图的触发条件。当前上述三种捕获计算图的方法都存在，由于网络结构并没有过于难处理的结构，所以在构建计算图的过程中使用符号追踪的方案。</p>
<h3 id="torch.fx中间表示方式">torch.fx中间表示方式</h3>
<p>构建的计算图有一些比较重要的参数，主要可以分为几类，一类表示的是与操作相关的数据如操作类型opcode，输入参数args，一些固定参数kward，操作使用的函数target，还有一类是用于表示计算结构的数据，如name表示输出名字，args表示所需输入名字，args里面的参数就是和name相关的，通过这两组数据就建立计算图的DAG。这里重点介绍一下opcode以及target参数。</p>
<p>opcode主要有6中，分别是</p>
<ul>
<li>placeholder用于表示计算图的输入</li>
<li>get_attr表示当前节点的操作是用来获取某些模块中的参数，例如在网络中有有操作单独将网络中某一层模块中的权值拿出来参与计算，这就是get_attr操作</li>
<li>call_function表示调用了一些python中自带的函数，其target就表示其将使用的操作是什么，例如两个整除的加法操作。</li>
<li>call_module表示其调用的方法是网络中的一些层操作，target表示的就是相关的操做名称在模型中模块的名字，例如卷积操作。</li>
<li>call_method表示调用torch函数，这个和call_module不同，函数是一些没有参数的模块。</li>
<li>output表示网络输出的是哪些东西。</li>
</ul>
<p>target表示的是节点计算使用的函数，如果数据计算使用的是网络中的模块名，target也会相应的与此模块名一致，同时opcode是call_module或call_method。如果调用是python内置的函数target会是相应的函数名与网络模块无关。</p>
<h3 id="torch.fx修改网络时的使用流程">torch.fx修改网络时的使用流程</h3>
<p>symbolic tracing -&gt; intermediate representation -&gt; transforms
-&gt; Python code generation 1. 获取计算图</p>
<p>首先使用符号追踪将网路的计算图拿到，计算图的结构如同一个链表一样，后续将这个链表修改。</p>
<ol start="2" type="1">
<li>修改计算图中节点，并修改模型相应的层</li>
</ol>
<p>这步是修改网络中的重点，利用拿到的计算图，根据自己写好的规则进行节点替换或增加节点，如果替换节点或增加节点的操作使用的是pytorch中的一些网络模型，那么在目标神经网络中还要添加相应的层，防止target指向的对象不正确。</p>
<ol start="3" type="1">
<li>检查计算图</li>
</ol>
<p>网络计算图修改之后，可能会因为粗心发生网络拓扑结构出现一些环状等不符合DAG条件的结构，所以需要调用相应的api对其进行计算图的检查，检查合格之后会进入最终代码生成阶段。</p>
<ol start="4" type="1">
<li>生成目标代码</li>
</ol>
<p>使用torch.fx的最终阶段就是代码的生成，torch.fx的工作方式和函数不一样，他不会像生成一个对象那样，后续去调用对象即可，而是选择生成python源码来进行最终的计算，</p>
<h2 id="torch.fx实现量化蒸馏方案">torch.fx实现量化蒸馏方案</h2>
<h3 id="方案配置">方案配置</h3>
<p>修改配置方案时，主要是通过网络中的层名来配置相应层的一些参数，但是由于网络中在定义层和定义计算流程的过程是分离的，一些层的定义之后可能会使用多次，这对量化和蒸馏是有一些麻烦的，例如得到relu层输出进行蒸馏或者对某一层之后的activation进行特殊量化处理会有些定位难得问题，这里的处理思路是首先对复用的一些层进行复制，修改计算图中使用复用层节点的target名称，然后将网络中相应的层名字进行新增，由于网络图进行了修改，这里通过对模型生成onnx模型再使用Netron去可视化网络，然后找出需要进行蒸馏以及特殊量化的层。写入配置文件。</p>
<h3 id="torch.fx实现蒸馏">torch.fx实现蒸馏</h3>
<p>本工作主要使用的是特征的蒸馏，所以需要输出一些中间特征，主要方案是修改网络中的output操作，通过yaml配置文件中的输出层名字来确定输出的层。</p>
<p>蒸馏需要两个网络都输出网络中间层的特征。一种是对量化之后网络层特征的输出，另一种是对未量化的网络层特征的输出。对于如上功能有两种设计方案，一种是量化时借用其符号追中进行量化的处理，然后非量化网络重新使用符号追踪再次进行蒸馏处理。另一种方案是单独设计一个模块，将量化网络和非量化网络都视为一类网络，然后重新进行符号追踪和蒸馏层输出。显然第二种方法的工作量和结构耦合是最小的，所以蒸馏单独成一个模块进行设计。</p>
<p>至于蒸馏层位置的选择如何处理，主要是用hessian矩阵进行处理，我们会在后文进行交代</p>
<h3 id="torch.fx实现量化">torch.fx实现量化</h3>
<h4 id="量化模型加载原模型参数">量化模型加载原模型参数</h4>
<p>根据前文所述，量化过程中想要达到最好的结果最好需要加载原模型来重新训练，以往的做法需要首先实现一个量化相关的层然后，层中使用一个开关，首先进行无量化训练，达到最好的结果在进行量化训练，但是这样的训练过程较为麻烦，同时也无法将原模型中参数加载到量化模型中。但是使用torch.fx对计算图操作是可以达到这样的效果的，再量化之前先进行原模型的加载，在进行层替换的时候首先将网络中的参数进行调换，然后再修改计算图，就完成了量化模型加载了原模型的参数。</p>
<h4 id="量化中的算子优化">量化中的算子优化</h4>
<p>在神经网络中，算子是最小的调度单位，但是算子并不具有原子性，一个复杂算子可以由多个简单算子组成。在量化推理的过程中，有时会使用类似于TensorFlow静态图优化的方法进行算子的优化。算子融合的目的主要有几个
1. 适配开发板计算算子</p>
<p>一些开发板对于神经网络提供的框架并不是完全支持，例如卷积算子，在开发板上并没有相应的算子，但是开发板支持矩阵操作，所以可以将卷积转化为相应的多个矩阵操作即可。</p>
<ol start="2" type="1">
<li>进行计算优化</li>
</ol>
<p>计算优化也分为两种，一种是适用于特定硬件的优化专用优化，另一种是对大多数硬件都适用的通用优化。</p>
<ul>
<li>通用优化</li>
</ul>
<p>这部分优化不考虑硬件，主要工作是对计算图相应的结构进行搜索然后将其替换为更优的结构。深度学习的算子可以分为两种：计算密集型算子和访存密集型算子，这也是大部分计算机中计算操作的类型，计算密集型算子其绝大多数时间花费在计算上，例如卷积和全连接层，虽然这两个算子需要访问其weight参数，但是其需要做矩阵计算操作，这部分的计算时间是远远大于访存时间的。访存密集型算子，这些算子的时间绝大多数花费在访存上，例如ReLU，其计算的时候先加载数据然后通过判断矩阵的值就能得到之后的结果，所以操作的主要耗时出现在访存上，一把来讲在设计网络的过程中，多半的操作都是计算密集型和访存密集型操作前后使用，利用两种结构的特点，我们可以将计算密集型的结果放在内存中，之后访存密集型的结构就不需要再去读数据直接使用上一步的操作数进行计算。</p>
<p>一类结构就是使用Conv +
ReLU，ReLU是访存密集型算子，Conv是计算密集型算子，这里可以将Conv计算结果直接交给RelU使用，两个算子就合并为一个算子了，这样就减轻了ReLU的访存压力，提高了计算效率。
<img src="Conv_Relu_fuse.png" alt="Conv_Relu_fuse" /></p>
<p>神经网络中一些算子的计算实际上也可以进行数学上的合并，比如Conv +
BatchNormal，也能进行层融合，不过BatchNormal的层融合需要分析一下BatchNormal的计算方式来进行融合。
在训练时BatchNormal中的计算方式如下： <span class="math display">\[y_i =
\gamma \frac{x_i - \mu}{\sqrt{\sigma ^2 + \varepsilon}} +
\beta\]</span></p>
<p>其中<span
class="math inline">\(\varepsilon\)</span>是一个防止除0的小量，<span
class="math inline">\(x_i\)</span>表示的是BatchNormal的输入数据，<span
class="math inline">\(\gamma、\beta\)</span>是BatchNormal中的可训练参数，通过梯度下降法来确定参数，<span
class="math inline">\(\mu\)</span>和<span
class="math inline">\(\sigma\)</span>表示的是根据x的均值和方差迭代计算出的，公式为：</p>
<p><span class="math display">\[
\mu_k = \lambda\mu_{k - 1} + (1 - \lambda) \mu_x
\]</span> <span class="math display">\[
\sigma_{k}^2 = \lambda\sigma_{k - 1}^2 + (1 - \lambda) \sigma_x^2
\]</span></p>
<p>为这个时候的计算不好进行优化，因为<span
class="math inline">\(\mu,\sigma,\gamma,\beta\)</span>一直在进行变化。观察推理阶段BatchNormal做了哪些操作：</p>
<p><span class="math display">\[y_i = \gamma \frac{x_i -
\mu}{\sqrt{\sigma ^2 + \varepsilon}} + \beta =
\frac{\gamma}{\sqrt{\sigma^2 + \varepsilon}} x + (\beta -
\frac{\gamma\mu}{\sqrt{\sigma ^ 2 + \varepsilon}})\]</span></p>
<p>将上式子进行修改，可以得到如下表达式：</p>
<p><span class="math display">\[
y = ax + b;
\]</span>
因为训练阶段参数已经确定，a,b才是个定值，这个时候进行优化才能真正的优化计算速度。这里分析上式子的x，假设其由卷积得来，那么有：</p>
<p><span class="math display">\[
x_i = w_1 \cdot z_1 + w_2 \cdot z_2 + w_3 \cdot z_3 + bias
\]</span> 其中<span
class="math inline">\(w_i\)</span>表示卷积中的权重，bias表示的是偏置值，z_i表示的是输入值，将上式与BatchNormal计算公式进行联立可得：</p>
<p><span class="math display">\[
y_i = ax_i + b = a(bias + \sum w_i \cdot z_i) + b;
\]</span> 变形之后会得到： <span class="math display">\[
y_i = ax_i + b = (\sum aw_i \cdot z_i) + (b + a \times bias) ;
\]</span></p>
<p>可以看出，BatchNormal在与Conv进行合并的过程中，需要将卷积的权值扩大a倍，并将bias改为<span
class="math inline">\(b + a \ times
bias\)</span>即可完成两个层的合并。</p>
<figure>
<img src="Conv_BatchNormal_fuse.png" alt="Conv_BatchNormal_fuse" />
<figcaption aria-hidden="true">Conv_BatchNormal_fuse</figcaption>
</figure>
<p>，本工作量化框架主要使用了这两个常用的优化的方案。</p>
<ul>
<li>专用优化</li>
</ul>
<p>这种优化需要从开发板角度考虑，将板子中提供的一些算子加入和网络中的一些算子进行相关转化，以此达到最快的计算速度，这并不在本工作的工作范围内，所以并不考虑。</p>
<p><a
target="_blank" rel="noopener" href="https://openmlsys.github.io/chapter_backend_and_runtime/graph_optimizer.html"></a>
<a
target="_blank" rel="noopener" href="http://acodespace.com/archives/%E6%B5%85%E5%B0%9D%E8%AE%A1%E7%AE%97%E5%9B%BE%E4%BC%98%E5%8C%96%E7%AE%97%E5%AD%90%E8%9E%8D%E5%90%88"></a></p>
<h4 id="量化层参数的配置">量化层参数的配置</h4>
<p><a target="_blank" rel="noopener" href="https://pytorch.apachecn.org/1.4/88/"></a></p>
<p>神经网络使用8位整数进行计算，但是在一般平台中，用于计算的寄存器并不是8位的，因为需要考虑8位运算的溢出问题，所以寄存器位数会稍微大一些，所以一些平台也可以使用高于8位的计算，为了支持这种功能，这里量化中首先支持了量化位数的选择。其次，量化中需要考虑一些层量化的配置，例如一些层的权重需要使用逐通道量化，一些层需要使用对称量化，同时需要支持不同的量化方法选择，所以在量化参数选择上，量化框架给出了配置默认量化位数、量化算法、对称量化以及逐通道量化的字段，同时还支持特殊层量化参数的配置，只需要给出相应的量化层即可。</p>
<h2
id="torch.nn.qat与torch.nn.intrinsic.qat实现量化神经网络">torch.nn.qat与torch.nn.intrinsic.qat实现量化神经网络</h2>
<p>由于网络中有好多类算子，如卷积、全连接，这些类算子还有许多类型，如2d卷积，3d卷积，反卷积等等，如果想实现一个量化算法，那么需要将所有操作都实现成量化算子，同时现在量化方法有很多，那么这就会导致实现一个量化框架的工作量会成倍的增长，幸好pytorch提供了torch.nn.qat与torch.nn.intrinsic.qat算子库，在量化的过程中只需要将量化方案写好相应的类，然后其可以自动将权重进行量化，</p>
<h3
id="torch.nn.qat与torch.nn.intrinsic.qat的使用">torch.nn.qat与torch.nn.intrinsic.qat的使用</h3>
<p>在torch1.8版本中，torch.nn.qat中主要实现了相关的量化模块，torch.nn.intrinsic中实现了这Conv和Linear与其他算子融合的操作，并且再torch.nn.intrinsic.qat提供了融合之后的量化模块，这些算子都在32位浮点数精度下运行，使用了舍入方式来模拟量化效果，可以很好的进行量化感知训练，两个模块已经足够应付大部分典型的CNN与RNN网络了。下面给出两个模块中主要实现的算子：</p>
<p>torch.nn.qat - Linear : 全连接量化感知训练算子 - Conv :
卷积量化感知训练算子</p>
<p>torch.nn.intrinsic.qat</p>
<ul>
<li>ConvBn2d : Conv2d + BatchNormal融合的量化感知训练算子</li>
<li>ConvBnReLu2d : Conv2d + BatchNormal +
ReLU融合的量化感知训练算子</li>
<li>ConvReLU2d : Conv2d + ReLU融合的量化感知训练算子</li>
<li>LinearReLU : Linear + ReLu融合的量化感知训练算子</li>
</ul>
<p>上述算子的使用和正常未量化神经网络中的代码一样，不同的是，相比于原始算子，量化算子需要设置qconfig变量来配置权重参数的量化方案。</p>
<p>这里的模块实际上是需要配合torch.fx中的工具进行使用的，因为需要将原始网络中的未量化的算子替换为量化算子，即可完成量化模型的生成。</p>
<h2 id="量化算法的组成">量化算法的组成</h2>
<p>量化感知训练由两部分组成，首先是两户参数的统计，主要是对activation进行统计，在统计结束之后才进行相应的量化，所以在开发量化算法时也有两部分组成，一部分是参数量化，另一部分是参数统计，在代码中都放在相关层的qconfig变量中，在上一节所说的量化中，torch.nn.qat与torch.nn.intrinsic.qat只表示了对weight进行量化的操作，但是没有对activation量化的操作，所以，在生成量化网络中需要我们手动在量化节点之前加上activation量化操作，这样才能完成整个量化算子。
<img src="qconfig.png" alt="qconfig" /></p>
<p>在框架的组织上，需要将统统计算法和量化算法分开进行开发，将两部分解耦合可以很好的进行量化算法和参数统计算法组合，同时在量化算法中还要加入一个开关，用于切换参数统计和量化两种计算模式的切换。</p>
<h2 id="量化框架通用性展示">量化框架通用性展示</h2>
<p>既然是一个量化框架，必然可以适用于多种网络，这里我们以经典的网络为例，来展示框架的易用性，这里测试一下经典结构unet结构的网络，这里采用一个使用unet结构的语义分割网络进行测试。</p>
<p>unet是一种常用在计算机视觉中的网络结构，最早用于解决医学中的细胞分割的问题，所以常见的一些语义分割会使用unet，当然本文网络中实际上也使用了unet，unet在2015年的ISBI
cell
tracking比赛中获得了很多方面的第一名。unet网络结构主要是卷积和池化层相互交叉，使用了一种类似于Encoder-Decoder的结构，如下图所示
<img src="unet-encoder-decoder.png" alt="unet-encoder-decoder" /></p>
<p>可以看到网络结构主要由卷积和池化等操作组成，左侧为下采样网络，右侧为上采样网络，图中中间的四个水平线表示的是，主要用来融合上采样和下采样之间的特征。融合操作就是将特征图的通道进行叠加。
根据前面的介绍，模型量化过程中需要将节点进行替换操作，这里可视化这种替换操作情况可以使用Netron查看网络模型情况的工具，将网络转换为onnx模型，可以很清楚的看出模型使用了什么层，对那部分进行了转换，如果无法转换说明量化时没有办法进行正常计算。</p>
<h3 id="量化模型结果">量化模型结果</h3>
<p>首先查看框架对于量化节点的处理，然后再查看对于蒸馏处理的情况，同时查看量化框架的灵活，对于激活值和权重量化方法以及相应位数的修改是否支持。
1. 量化情况 <img src="no_quant_method.png" alt="no_quant_method" /> <img
src="quant_method.png" alt="quant_method" /></p>
<p>可以看到量化网络和未量化网络之间差距，量化网络中使用了量化节点来替换原来未量化的节点！</p>
<ol start="2" type="1">
<li>蒸馏情况</li>
</ol>
<p><img src="quant_distill.png" alt="quant_distill" /> <img
src="no_quant_distill.png" alt="no_quant_distill" /></p>
<p>当前网络位置是在最后一个位置，这里量化网络中的1053表示的是最终网络输出的结果，与为量化结果中的307号节点一致，量化网络的1048节点与量化网络的1053节点一样，同样是表示输出节点，但是这是蒸馏框架后加入的。可以看出量化功能是正常的。</p>
<ol start="3" type="1">
<li>量化参数相关</li>
</ol>
<p>可以看到设置的量化方法有两个，对任一层设置任意量化函数这个功能是成功的，同时可以看到其中含有参数0和1023，这个是表示量化后可取整数值为<span
class="math inline">\([0,1023]\)</span>，可以看到这个层设置的量化是逐张量量化并且量化位数为10位。
<img src="quant_method_config.png" alt="quant_method_config" /></p>
<h1 id="本文主要使用的方法">本文主要使用的方法</h1>
<h2 id="暗光增强方法">暗光增强方法</h2>
<p>选用贴合实际的一个工作，使用raw图片来进行处理。</p>
<h3 id="暗光网络结构">暗光网络结构</h3>
<p>本工作的量化是在Abandoning the Bayer-Filter to See in the
Dark工作基础上来完成的，这里介绍一下这个工作的一些细节。</p>
<h3 id="暗光增强处理思路">暗光增强处理思路</h3>
<p>在数字图像中，室内和夜间等照明不佳的环境，以及曝光参数不合适的条件下，由于颜色失真和噪声等原因图像质量会下降。加长曝光时间通常会导致运动模糊等问题，同时采用相机补光会产生颜色失真等问题，近些年来深度学习方法已经用于解决暗光增强问题。然而一些深度学习算法可能受到一些硬件的限制，因为不同的硬件其接收光子的能力是不同的，比如RYYB色彩滤波阵列比RGGB色彩滤波阵列多接收40%的光子，因此运行在拥有RYYB色彩滤波阵列的算法会有更好的性能。
为了解决这个问题，可以再捕获光子时将色彩滤波阵列去除，这样通过牺牲颜色信息来获取更多的光子数量，与带有色彩滤波器的设备相比会获得更好的成像效果。实际上一些智能手机现在就是使用这样的方法去完成摄像质量的提升，双摄像的智能机会配备一个单色传感器摄像头和一个彩色传感器摄像头，其中的单色传感器摄像头没有色彩滤波阵列，其他方面与彩色传感器摄像头一致，从而能接受更多光子，这种双摄像头的配置可以再暗光环境下获得更好的拍照效果。但是相机数量也是一种成本，一些设备实际上并没有多个摄像头。
借用上述的想法，可以使用两个模块来模拟上述处理方案：De-Bayer-Filter(DBF)模块使用彩色传感器的摄像头来计算出单摄像头的输出，结果作为单色图像的预测，从而代替单色相机，Dual
Branch Low-light
Enhancement(DBLE)模块将彩色输入和模拟出来的单色输出通过一个双分支处理模块，融合生成最终的RGB暗光增强图。弥补了有色raw图与单色raw图的不足，同时，融合的过程中使用了通道注意力机制，用来建立两种数据之间的交互，从而获得更好的回复能力。</p>
<h3 id="网络结构介绍">网络结构介绍</h3>
<p>本文的网络使用相机的raw数据进行处理，传统isp的处理方式raw数据的方式主要是通过几个算法：白平衡、demosaic、降噪、锐化、颜色空间转换、Gamma矫正。
<img src="isp_calculate_raw.png" alt="isp_calculate_raw" /></p>
<p>但是在本文中主要使用深度学习方法来处理暗光raw图像，防止isp在处理过程中将图像原始信息删除掉，使之能取得更好的性能，</p>
<h4 id="de-bayer-filter模块">De-Bayer-Filter模块</h4>
<p>数百万的微小传感器用于接受光子并将其转化为电信号，单独的传感器只能产生会的图像，bayer滤色镜覆盖在传感器之上收集颜色信息来产生彩色图像，一个标准的bayer单元主要的组成为<span
class="math inline">\(2 \times
2\)</span>的带有两个绿色一个红色和蓝色的滤色镜，一个确定滤色镜只会让相应波长的光子通过，现在的神经网络可以对相机成像过程进行很好的模拟<a
href="Learning%20to%20see%20in%20the%20dark.%7CNeural%20camera%20simulators%7CDeepisp:Toward%20learning%20an%20end-to-end%20image%20processing%20pipeline"></a>。在这里使用了一个De-Bayer-Filter(DBF)模块，用于滤色镜对光线处理中光子量变化的建模，以得到去除bayer滤色镜之后的结果。DBF模块将输入的原始彩色raw图像<span
class="math inline">\(A_{color} \in R^{\frac{H}{2} \times \frac{W}{2}
\times 4}\)</span>恢复为单色图像<span class="math inline">\(A_{mono} \in
R^{H \times W}，模块可以公式化为：\)</span>$ A_{Mono} = f_{M}(A_{Color})
$$ 其中f_{M}()是一个基于unet的卷积网络结构，使用L1损失函数去来对<span
class="math inline">\(f_{M}\)</span>网络进行训练，这里认为f_{M}产生的单色图像会给后续暗光增强任务提供更多信息。</p>
<figure>
<img src="model_struct.png" alt="model_struct" />
<figcaption aria-hidden="true">model_struct</figcaption>
</figure>
<h4 id="双分支暗光图像增强模块">双分支暗光图像增强模块</h4>
<p>raw彩色图像和单色图像有很多不同点 1. raw彩色图像有bayer结构 2.
raw彩色图像由四个通道组成，每个通道的分辨率为<span
class="math inline">\(\frac{H}{2} \times
\frac{W}{2}\)</span>，所对应的图像是一个分辨率为<span
class="math inline">\(H \times W\)</span>的图像。 3.
单色图像中不包括颜色信息。 4.
单色相机传感器可以更好的捕捉光线，单色图像更好的保留了图像照明信息</p>
<p>双分支暗光增强模块DBLE（dual branch low-light image
enhancement）基于以上特征进行设计，其用来处理raw彩色图像以及DBF产生的单色图像，DBLE首先对两个输入进行下采样，然后在级联的基础上对下采样的特增进行融合，同时在上采样分支上加入通道注意力层（CA），来合成符合人眼视觉的RGB图像<span
class="math inline">\(I_{rgb} \in R^{H \timesW \times
3}\)</span>，模块可以公式化为： <span class="math display">\[I_{RGB} =
f_{C}(A_{Color};A_{Mono})\]</span> 其中<span
class="math inline">\(f_{C}\)</span>表示全卷积神经网络，ground
truth与预测图像使用L1范数来计算损失，以此来训练DBLE从中学习到将低光照图像恢复人眼可接受的正常光照的RGB图片。</p>
<p>传统的unet网络，对待每一个特征是平等的，所以不能直接融合单色图特征和彩色图特征，否则会导致一些特征的矛盾。同时转置卷积以及卷积步长的设置也会将空间信息进行忽略，于是在级联之后会使用通道注意力机制来对结果进行校准，以弥补彩色图像和单色图像之间的差距，通道注意力机制可以很好的对彩色图像和单色图像的相互作用进行建模，使得二者互补来减少两个领域的矛盾。当然通道注意力机制的作用不止这些，unet还有可能会导致棋盘状失真，通过通道注意力机制可以很好的限制这样，因为通道注意力机制中也包含放大缩小等操作，因此通道注意力机制类似于通过权重重新绑定的方式来抑制此类事件的发生。</p>
<h3 id="数据集介绍">数据集介绍</h3>
<p>本工作使用了SID（See-in-the-Dark）数据集，SID数据集由Learning to See
in the
Dark这篇论文提供，其主要目的是为了服务于使用raw图像暗光重建任务而产生的，同时也为一些降噪算法进行服务。</p>
<p>SID数据集包括5094张raw短曝光图像，每一张短曝光图像都有一张与之对应的长曝光图像，但是一张长曝光图像与多个不同曝光时间的短曝光图像对应，因为短曝光图像可能会有一些噪声，所以这个数据集也被用来测试降噪算法，同一场景下每一张短曝光图像都是不同的，因为他们有不同的伪影。不同的长曝光图片的数量有424张。
数据集包含了室内和室外的场景，室外场景是在夜间伴这月光或者路灯拍摄所得，在室外场景下光照强度通常在0.2lux到5lux之间。室内场景光照强度接近于黑暗，通常在封闭环境、没有灯光的条件下下进行拍摄，室内场景下光照强度通常在0.03lux到0.3lux之间。</p>
<p>数据集中短曝光图片的曝光时间设置在了0.03 ~ 0.1秒之间，相应的ground
truth，也就是长曝光时间在10到30秒，是短曝光时间的100到300倍，所以相比于暗光图像，长曝光图像曝光的时间是足够长的。数据集中的所有场景都是静态的，不然会有一定程度的动态模糊，具体情况见下表，具体数据情况见下图
<img src="dateset_detail.png" alt="dateset_detail" /> <img
src="long_expose.jpg" alt="long_expose" /> <img src="short_expose.jpg"
alt="short_expose" /></p>
<p>图像的拍摄主要是使用了<span class="math inline">\(Sony
\alpha7S\)</span>和<span class="math inline">\(Fujifilm
X-T2\)</span>相机，两个相机有着不同的传感器，Sony相机使用了全画幅的Bayer传感器，Fuji相机使用了APS-C
X-Trans传感器，相机固定在三脚架上，并使用无反光镜相机来避免反光镜抖动产生的振动，在不同场景中都要调节相机的一些参数，以此来获得最佳的长曝光图像效果。短曝光图像使用智能手机使用长曝光时间减少100到300倍之后的时间长度进行拍摄。将拍摄后能完美对齐的图像进行使用。
虽然长曝光图像含有一些噪声，但是其足以作为ground
truth进行训练，因为网络主要目的是为了暗光增强效果。</p>
<h2 id="量化方案">量化方案</h2>
<h3
id="如何解决图片颜色分层的问题以及亮点出现黑洞的问题">如何解决图片颜色分层的问题，以及亮点出现黑洞的问题</h3>
<p>主要是由于在网络中8位整数量化</p>
<h4 id="中间增加位数">中间增加位数</h4>
<h4 id="两端增加位数">两端增加位数</h4>
<h3
id="量化方法中使用dsq-与-lsq方法进行神经网络训练更好的将网络调参进行下去">量化方法中使用dsq
与 lsq+方法进行神经网络训练。更好的将网络调参进行下去</h3>
<p>由于一些时候，一些非深度学习的量化参数调节方法未必回答道很好的效果，所以这里使用神经网络自主调整量化参数的方法lsq+，首先使用确定的算法统计出一个参数，然后这个参数在经过神经网络进行调节，同时为了能使得lsq+更加准确，对反向传播过程这里不再使用传统的STE去模拟反向传播过程，而是用更符合数学推理的dsq方式近进行推理。为了能更好的理解本文的方法，首先对lsq+的简单版本lsq方法以及lsq+方法和dsq进行介绍。</p>
<h4 id="lsq方法">lsq+方法</h4>
<p>为了能更好的理解lsq+方法，这里使用lsq来对lsq+方法的数学推理先进性铺垫。
深度神经网络在推导过程中使用低精度整数操作计算卷积和全连接层，对于卷积和全连接层需要量化其中的权重和激活值，当给出需要量化的参数为x，量化中scale为s，量化后整数边界设为<span
class="math inline">\(Q_{P}\)</span>和<span
class="math inline">\(Q_{N}\)</span>，可以定义一个量化公式，使之得到最终量化之后的结果<span
class="math inline">\(X\)</span> <span class="math display">\[
X = \lfloor clip(\frac{x}{s},-Q_{N},Q_{P}) \rceil \\
\]</span> 其中clip(a,l,r)表示当a &lt; l时函数取值为l，a &gt;
r时函数取值为r，否则取值为a。<span class="math inline">\(\lfloor a
\rceil\)</span>表示输出距离a最近的整数，使用这样的一个方案视为和后续求导的一些问题，这个在后文介绍。假设量化位数为b，量化使用对称量化，那么<span
class="math inline">\(Q_{N} = 2^{b - 1}\)</span>，<span
class="math inline">\(Q_{P} = 2^{b - 1} - 1\)</span>。</p>
<p>有了量化方案，那么剩下的就是怎样确定量化方案中参数，现已知量化方案中未知的参数为s，所以要通过一些方法确定s，LSQ给出的方法是通过网络计算loss值后进行反向转播学习s，但是由于损失函数在对s求导时并不可导，所以需要手动给出s的导数求解函数，主要参考STE方法进行相应的推导。现给出推导过程！！</p>
<p>首先分析STE的导数求解方案，对于取整函数没有导数的问题，STE将取整函数的导数进行修改了，这在pytorch上很容易实现，只需要给出一个函数为取整函数，其backward函数为函数的导数计算函数，这样就能按照预想的方案来求解梯度，STE将取整函数的梯度设置为如下表达式：
<span class="math display">\[
\frac{\partial \lfloor x \rfloor}{\partial x} = 1
\]</span>
表达式十分简单。将上述方案应用在LSQ的量化表达式上会有一些问题，在训练过程中，通过量化和反量化将量化损失引入网路中，假设X'为反量化之后的结果,X为量化之后的结果，s为量化时区间映射的系数，则<span
class="math inline">\(X&#39; =
Xs\)</span>，于是X'关于s的导入有如下推导： <span class="math display">\[
\begin{matrix}
\frac{\partial X&#39;}{\partial s} &amp;= \frac{\partial Xs}{\partial
s}\\
                               &amp;= s\frac{\partial X}{\partial s} + X
\end{matrix}
\]</span></p>
<p>其中X关于s的导数如下： <span class="math display">\[
\frac{\partial X}{\partial s} = \frac{\lfloor
clip(\frac{x}{s},-Q_{N},Q_{P}) \rceil}{\partial s}
\]</span>
由于clip函数的存在，所以上式是一个分段函数，所以在这里分段讨论，当<span
class="math inline">\(-Q_{N} &lt;= \frac{x}{s} &lt;=
Q_{P}\)</span>时<span
class="math inline">\(clip(\frac{x}{s},-Q_{N},Q_{P}) =
\frac{x}{s}\)</span>，即求导函数为 <span class="math display">\[
\frac{\partial X}{\partial s} = \frac{\lfloor
clip(\frac{x}{s},-Q_{N},Q_{P}) \rceil}{\partial s} = \frac{\lfloor
\frac{x}{s}\rceil}{\partial s}
\]</span> 类比STE函数，<span
class="math inline">\(\frac{\partial{\lfloor \frac{x}{s}
\rceil}}{\partial\frac{x}{s}} = \frac{\partial{\lfloor \frac{x}{s}
\rceil}}{-\frac{x}{s^{2}}\partial s} = 1\)</span></p>
<p>所以 <span class="math display">\[
\frac{\partial X}{\partial s} =
\frac{\partial\lfloor\frac{x}{s}\rceil}{\partial s} = -\frac{x}{s^2}
\]</span></p>
<p>当<span class="math inline">\(\frac{x}{s}\)</span>不在<span
class="math inline">\([-Q_{N},Q_{P}]\)</span>中，X为与S无关的常数，导数为0，所以X’关于s的导数为：</p>
<p><span class="math display">\[
\frac{\partial X&#39;}{\partial s} = \left\{\begin{matrix}
\lfloor \frac{x}{s}\rceil - \frac{x}{s}\\
-Q_{N}\\
Q_{P}
\end{matrix}\right.
\]</span>
以上就是LSQ在训练中使用的梯度函数的推导过程以及结论。观察导数函数，可以看到<span
class="math inline">\(\frac{x}{s} - \lfloor \frac{x}{s}
\rceil\)</span>表示的是取整之后和美取整之后的差距，使用<span
class="math inline">\(\lfloor\rceil\)</span>可以使得梯度函数绝对值最值比较小，如果使用<span
class="math inline">\(\lfloor\rfloor\)</span>会使得梯度函数绝对值最值更大，可以观察如下函数图像，如图：
<img src="grad_graph.png" alt="grad_graph" />
在LSQ方法中s参数在初始化时每个激活值以及权重都有不同的初始值，一般固定为<span
class="math inline">\(s = \frac{2&lt;|x|&gt;}{\sqrt{Q_{P}}}\)</span>
事实上还有一些其他的通过神经网络量化思路来解决该问题的方法，如QIL，PACT等，本工作选择了LSQ原因主要还是在于导数表达式上。为了能更好的说明LSQ的效果，这里简单的介绍一些QIL，PACT方案的工作原理</p>
<p>PACT的思路主要针对activation而言，在量化的过程中权重和激活值不一样，权重的量化损失可以通过反向传播进行训练来补偿，但是激活值的量化损失只能再次通过权重进行补偿。ReLU函数有很多变体，比较原始的ReLU是没有上界的，如果网络中使用这样的激活函数，那么最终获得的激活值也是没有上界的，如果激活函数的输出能在一定范围内，这样得到的激活值的量化误差会相应小一些，截断式ReLU就是这样的函数，但截断ReLU虽然能缓解量化误差，但是却不能考虑到真正的激活值分布情况，简单的使用也会造成一些问题。PACT方法是一种带有学习参数的截断式ReLU函数，其方程为</p>
<p><span class="math display">\[
y = PACT(X) = 0.5(|X| - |X - \alpha| + \alpha) = \left\{\begin{matrix}
0, x\in(-\infty, 0)\\
x, x\in[0,\alpha)\\
\alpha, x\in[\alpha,+\infty)\end{matrix}\right.
\]</span></p>
<p>如果<span
class="math inline">\(\alpha\)</span>是一个固定参数，那么其输出为与截断式ReLU一致，但是PACT为了使其能更好的缓解激活值量化误差补偿问题，将<span
class="math inline">\(\alpha\)</span>设计为一个网络中的可学习参数。于是激活值的量化函数可以写为：
<span class="math display">\[
X = \lfloor x\frac{2^{k} - 1}{\alpha}\rceil
\]</span> 相应的反量化函数为 <span class="math display">\[
X&#39; = X \frac{\alpha}{2^{k} - 1}
\]</span> 这里的量化的step为<span
class="math inline">\(\frac{\alpha}{2^{k} -
1}\)</span>，与LSQ类似也是一个可学习参数。</p>
<p>这里对反量化参数关于可训练参数进行求导，可以得到如下的结果 <span
class="math display">\[
\frac{\partial X&#39;}{\partial \alpha} = \left\{\begin{matrix}0,
x\in(-\infty,\alpha)\\1,x\in[\alpha,+\infty) \end{matrix}\right.
\]</span> 这里的梯度计算也是用STE技巧</p>
<p>QIL的思路与PACT类似，因为PACT修改使用截断式ReLU来规范激活值的范围，但是规范的范围是[0,]，很明显下界是不可调节的，这就导致了PACT算法有一些局限性，为了，弥补这样的问题，QIL使用下界可调节的方案，进一步将量化范围进行细化。梯度与PACT方法类似
如果对几种量化方法使用3位量化的方案，可以发现量化使用的取整函数，假设当前量化位数为3，观察图像
<img
src="https://pic4.zhimg.com/80/v2-ba2753f4c017927ddc9428597a38b3e7_720w.webp"
alt="quant_fun" /> 这个取整函数不是一个连续函数，可以发现其中在n +
0.5的位置函数发生了跳变，其中<span class="math inline">\(n\in
Z\)</span>，一种合理的梯度函数就是在n +
0.5的位置发生跳变，观察LSQ的求导函数可以发现其在0.5位置梯度发生了跳变，但是其他方法的导数只有有限的几个跳变位置，所以不符合数学逻辑。</p>
<p>LSQ在使用过程中只对量化中scale参数进行训练，但是其固定了量化时的零点，在LSQ+中改变了这样一个问题，将量化中的零点也作为量化参数进行训练，这就是LSQ与LSQ+之间的区别。</p>
<p>由上面的推导过程可知，对于取整函数使用了STE方案来近似导数，实际上这是不符合数学逻辑的，所以这里我们修改上述过程，将导数处处为0的取整函数改为使用可导函数模拟的方案来更加准确的将量化相关参数训练出来。</p>
<h4 id="dsq方法">dsq方法</h4>
<p>对于多位均匀量化方法中，量化函数的导数处处为0，使得网络在训练过程中无法找到真正的调整方向，使得量化之后的参数很难进行调节，为了缩小全精度模型与量化模型之间的差距，可以采用一种函数去模拟取整函数，在dsq中就使用了<span
class="math inline">\(tanh\)</span>函数进行模拟，对于多位量化中，由于取整函数有多个分段，但是tanh函数只有一个分段，所以需要使用多个tanh进行量化，对于量化过程中x所在的不同区间<span
class="math inline">\(P_{i}\)</span>其量化表达式为： <span
class="math display">\[
\varphi(x) = s\ tanh(k(x - m_{i})), if x\in P_{i}
\]</span> 其中 <span class="math display">\[
m_{i} = l + (i + 0.5)\Delta \\ s = \frac{1}{tanh(0.5k\Delta)}
\]</span>
其中的放缩参数s保证了tanh函数在不同区间能进行平滑的连接，由于tanh函数具有一定的对称性，所以<span
class="math inline">\(\varphi\)</span>函数在任意一个位置都可导，此外表达式中的系数k可以决定函数的形状是否接近量化函数，k值越大，渐进函数就越接进去取整函数的形式，整体呈现为阶梯函数。但是这种阶梯型形式是有一定范围的，加上一个范围之后，可以得到一个量化函数<span
class="math inline">\(Q_{s}\)</span> <span class="math display">\[
Q_{s} = \left\{\begin{matrix}l, &amp;x &lt; l,\\u, &amp;x &gt; u,\\l +
\Delta(i + \frac{\varphi(x) + 1}{2}),&amp;x\in P_{i}\end{matrix}\right.
\]</span></p>
<p>将这个函数图绘制之后可以发现 <img src="dsq_line.png"
alt="dsq_line" /></p>
<p>当<span
class="math inline">\(\varphi\)</span>中k值越大的时候，函数越像阶梯函数，所以在实际量化中，可以使用这个函数来模拟真实量化对于网络推理中结果的影响，同时在之前也提到，这个函数也能很好的进行反向传播的梯度计算，这个量化函数将函数进行整形，通过重新反向传播进行来准确的修正量化误差。</p>
<p>当<span
class="math inline">\(\varphi\)</span>的符合函数中出现了符号函数是，DSQ可作为均匀量化方法中的取整函数，当量化分段点只有一个时，这个时候的DSQ量化方法可以视为二值化量化方法。
<img src="dsq_split.png" alt="dsq_split" /></p>
<p>了解了DSQ函数之后，相应的对于神经网络标准量化可微函数可以使用dsq函数，但是dsq函数与量化取整函数的相似程度会影响量化结果，在训练的过程中需要对dsq中的参数进行合理的确定，才能达到训练的最佳效果。</p>
<p>为了能使得DSQ方法能达到最佳效果，原文中对DSQ中相关参数进行了实验，主要改变<span
class="math inline">\(\varphi\)</span>中的参数k，这里引入变量<span
class="math inline">\(\alpha\)</span>， <span class="math display">\[
\alpha = 1 - tanh(0.5k\Delta) = 1 - \frac{1}{s}
\]</span> 其中<span class="math inline">\(\Delta = \frac{u - l}{2^{b} -
1}\)</span>，可以理解这个<span
class="math inline">\(\alpha\)</span>实际上表示了在区间[-1,1]之间<span
class="math inline">\(\varphi\)</span>函数离1的最小距离。如下图所示：
<img src="alpha_means.png" alt="alpha_means" /></p>
<p>现在对上述表达式进行改写，使得相关参数使用<span
class="math inline">\(\alpha\)</span>与<span
class="math inline">\(\Delta\)</span>来进行表示，于是可以得到： <span
class="math display">\[
s = \frac{1}{1 - \alpha}
\]</span> 同时由于<span class="math inline">\(\varphi(0.5\varphi) =
1\)</span>，所以 <span class="math display">\[
k = \frac{1}{\Delta}log(\frac{2}{\alpha} - 1)
\]</span> 基于以上公式，文中对<span
class="math inline">\(\alpha\)</span>进行了改变，得到了量化最优解，量化过程中使用均匀量化，带符号函数的DSQ可以在均匀量化可以取得很稳定的量化结果，<span
class="math inline">\(\alpha\)</span>很小时，DSQ可以很等同于未使用DSQ的均匀量化。当然文中的任务是目标检测，所以可以得到的一个结论就是合适的<span
class="math inline">\(\alpha\)</span>可以使得量化效果更好，有助于提高模型精度。
根据以上结论，DSQ的逼近能力主要取决于<span
class="math inline">\(\alpha\)</span>，其在优化DSQ量化网络效果中起到了重要的作用，然而认为手动去调节这个参数显然有点不合适，也比较费力，为了能自适应的确定它，这里引入一种训练方式，该方式将量化网络中的<span
class="math inline">\(\alpha\)</span>视为更加重要的参数，并在训练的过程中对<span
class="math inline">\(\alpha\)</span>进行自适应调节，使其作为网络中的参数，于是量化网络的损失函数的目标可以使用如下表达式表示
<span class="math display">\[
min_{\alpha}L(\alpha;x,y)
\]</span> 其中x表示输入网络的数据，y表示ground
truth。根据这个表达式，可以通过求解L关于<span
class="math inline">\(\alpah\)</span>的导数，然后进行反向传播进行调节<span
class="math inline">\(\alpha\)</span>。</p>
<h4 id="dsq和lsq的结合">DSQ和LSQ的结合</h4>
<p>了解两种方案之后，这里给出将两种方法结合的一个方案，考虑到LSQ+虽然对量化相关参数加入到网络中称为网络训练参数来进行调节，但是调节所使用的数学方法实际上是不符合数学公式的，所以就算使用LSQ调节也未必能发挥这个思想的全部能力，DSQ却能很好的解决训练过程中出现的量化函数导数为0的问题。</p>
<p>算法的主要流程是这样的，首先将网络使用我们开发的框架进行量化，虽然LSQ能对量化参数进行神经网络训练，但是在量化之前仍然要进行量化相关参数的设置，使得网络尽可能接近最优解，主要是通过参数统计进行，通过量化网络对输入数据集的计算统计激活值的分布，通过激活值分布来确定对激活值量化的初始参数为多少，通过。对于底层量化取整函数采用dsq的方案，将多段反正切函数拟合取整函数，一次达到正确的梯度反向传播。</p>
<h2 id="量化蒸馏方案">量化蒸馏方案</h2>
<p>量化过程中量化网络产生了精度的偏差会使得网络很难达到预期效果，量化模型又可以视为一个压缩的模型，所以量化任务可以视为将一个压缩之后的模型训练接近原始未量化模型的任务。在以往的工作中，蒸馏是将原模型精简之后的模型，通过未精简的教师模型的一些辅助结果进行训练，在量化中也可以采用这样的思路，将量化后的模型视为学生网络，同时使用一个未精简的并且训练到收敛的教师网络进行辅助训练。</p>
<p>有了这样的思路之后仍然有一些问题需要解决</p>
<ol type="1">
<li>如何进行学生和教师网络的知识传递</li>
<li>教师网络蒸馏特征应该怎样选择</li>
<li>如何指定损失函数 在后文会对这些问题进行解决</li>
</ol>
<h3
id="学生网路和教师网路之间的知识传递">学生网路和教师网路之间的知识传递</h3>
<p>知识蒸馏传统的方法中对于知识的传递主要有，基于相应的知识蒸馏、基于特征的知识蒸馏、基于关系的知识蒸馏，由于本文网络训练中存在数据集，所以不需要基于相应的知识蒸馏，同时教师网络并不是很大，同时基于关系的知识蒸馏又较为复杂不实用。本文主要是做图像增强任务的，需要一些层学习到相应的特征，所以这里使用了基于特征图的知识蒸馏。知识蒸馏的一个关键点在于如何设定损失函数，使得其能够学到所要的特征，对一些概率特征的学习可以使用交叉生损失函数，对于特征图的学习可以使用MSE损失函数来，经过当学生模型的特征图接近教师模型的特征图是，MSE损失函数会接近于0。</p>
<h3 id="如何设定损失函数">如何设定损失函数</h3>
<p>由于不同特征对于结果的的好坏有不同的影响，所以不同特征的重要性也是不一样的，所有特征损失函数融到最终的损失函数之后的表达式结果如下：
<span class="math display">\[
L = \sum_{i = 1}^{n} \lambda_{i}L_MSE{t_i,s_i}
\]</span> 其中n表示学生网络向教师网络学习特征的数量，<span
class="math inline">\(t_i\)</span>表示的是教师网络中第i个特征，<span
class="math inline">\(s_i\)</span>表示的是学生网络中第i个特征，而<span
class="math inline">\(\lambda_i\)</span>表示的是特征的重要性，再传统基于特征的知识蒸馏中教师网络的不同位置的特征情况是不一样的，教师网络的顶层具有多的参数和更强的表达能力，可以捕捉高级特征，教师网络中间层可以获得更为具体的局部特征，但是这些选择方案缺少一些数学推到依据，同时无法确定哪些层具有较高的重要性，那些曾具有较低的重要性，所以在本文使用了hessian统计参数的评估方法，蒸馏层的确定以及蒸馏层的重要程度都需要根据hessian矩阵相关统计值来进行确定。</p>
<h3 id="如何选取蒸馏层">如何选取蒸馏层</h3>
<p>如果一个层发生一定的变动会对结果产生很大的影响，那么我们认为这一层的参数是一个非常重要的参数，通过遍历每一层的输出值然后改变输出参数大小观察对结果的影响，以此为依据来确定蒸馏层的选择确实是一个方案，但是这样的方法有几个问题：1.
如何衡量对不同矩阵的修改程度，2. 怎样快速的完成遍历。首先第一个问题
如何衡量矩阵的修改程度，假设矩阵变动之后与原矩阵的变动之差为<span
class="math inline">\(\Delta
W\)</span>，比较两个矩阵可以使用数学中的范数，我们使用<span
class="math inline">\(|\Delta W|^2_2\)</span>即<span
class="math inline">\(\Delta
W\)</span>的二范数来表示矩阵的变化程度。第二个问题需要我们通过固定这个二范数之后，找出一种数学方法来衡量其对最终输出结果的影响。本文主要使用的是hessian矩阵的迹来进行衡量，通过计算activation的hessian的迹来判断特征的重要程度。</p>
<h3 id="hessian矩阵介绍未完">hessian矩阵介绍(未完)</h3>
<p>hessian矩阵式是一个与函数二阶导数有关的矩阵，一个函数存在多个变量时，对所有参数进行二阶偏导数然后将其组成方阵，得到的矩阵就是hessian矩阵，假设函数形式为<span
class="math inline">\(f(x_{1},x_{2},...,x_{n})\)</span>其表达式为: <span
class="math display">\[H_{ij} = \frac{\partial^{2}f}{\partial x_{i}
\partial x_{j}}\]</span></p>
<p>更直观的讲可以为 <img
src="https://wikimedia.org/api/rest_v1/media/math/render/svg/23f4db415be866163432946603c07edbc4a21a41" /></p>
<h4 id="hessian矩阵和泰勒展开">hessian矩阵和泰勒展开</h4>
<p>hessian矩阵和泰勒展开有一定关系，当函数为一个多元函数时，其泰勒展开中会带有与之相关的hessian矩阵。</p>
<p>当一个一元函数<span class="math inline">\(y = f(x)\)</span>在<span
class="math inline">\(x =
x_{0}\)</span>上具有任意阶导数是，在此点进行泰勒展开会有如下的表达式：
<span class="math display">\[
f(x) = f(x_{0}) + f&#39;(x_{0})\Delta x +
\frac{1}{2}f&#39;&#39;(x_{0})(\Delta x)^{2} + ...
\]</span> 其中<span class="math inline">\(\Delta x = x -
x_{0}\)</span>。</p>
<p>现在对二元函数进行展开泰勒展开：</p>
<p>仿照一元函数的形式进行整理就会发现，一元函数在二阶导数的位置处在二元函数泰勒展开中对应的是hessian矩阵。</p>
<p>推广到多元函数中，多元函数的泰勒展开可表示为：</p>
<p><a
target="_blank" rel="noopener" href="https://www.zywvvd.com/notes/study/math/hessian-matrix/hessian-matrix/">参考</a></p>
<h4 id="hessian矩阵的性质">hessian矩阵的性质</h4>
<p>hessian矩阵是由二阶导数组成的，根据多元微分的理论，如果一个函数的二阶偏导数在某点连续，那么二阶偏导顺序可以交换。
<span class="math display">\[
\frac{\partial^{2}f}{\partial x_{i} \partial x_{j}} =
\frac{\partial^{2}f}{\partial x_{j} \partial x_{i}}
\]</span> 其中<span class="math inline">\(x_{i}\)</span>与<span
class="math inline">\(x_{j}\)</span>表示的是f中的变量。</p>
<p>根据hessian的形式可以知道，如果f在实数域，同时二阶偏导数连续，那么hessian将会是一个实对成矩阵。n阶实对称矩阵拥有n个正交的特征向量，所以hessian也有n个正交的特征向量。</p>
<h3 id="hessian矩阵选择迹">hessian矩阵选择迹</h3>
<p>最开始hessian矩阵的迹主要使用在多位量化中，用于给出低精度多位量化每一层位数的选择，其内部原理主要是根据每一层对于变化的敏感程度来判断的，这刚好能用在判断某一层特征的重要程度上。</p>
<p>对于一个监督学习框架，其主要目标可以定义为将损失函数的值降到最低，损失函数可以表示为：
<span class="math display">\[
L(\theta) = \frac{1}{N}\sum_{i = 1}{N}f(x_{i},y_{i},\theta)
\]</span> 其中<span
class="math inline">\(\theta\)</span>是模型中可学习权重，<span
class="math inline">\(f(x,y,\theta)\)</span>表示的是损失函数，由于当前为蒸馏层选择依据的推到，所以其中未包括本次蒸馏过程中所使用的损失函数。x表示模型的输入数据，y表示模型的输出结果，N表示的是训练集中图片的数量，假设一个神经网络可以分为L层<span
class="math inline">\({B_{1},B_{2},...B_{L}}\)</span>，这些层中对应的学习参数值为<span
class="math inline">\({W_{1},W_{2},...,W_{L}}\)</span>，假设一个batch中的数据两为<span
class="math inline">\(N_{B}\)</span>，那么一个batch中训练过程中模型参数的梯度为<span
class="math inline">\(g = \frac{1}{N_{B}} \sum_{i =
1}^{N_{B}}\frac{\partial f}{\partial
\theta}\)</span>，一个batch的训练过程中模型某一层的hessian矩阵为<span
class="math inline">\(H = \frac{1}{N_{B}} \sum_{i =
1}^{N_{B}}\frac{\partial^{2}f}{\partial \theta^{2}}\)</span>。</p>
<p>在这里先不考虑量化的问题，为方便后面的分析，假设模型是可训练的，所有网络参数和激活值都视为浮点数，不存在不可导的问题。接下来进行理论分析，研究网络中某一层发生改变之后会对整个模型造成什么影响，以确定某一层激活值的重要程度。</p>
<p>hessian能够衡量某一层变化对于损失函数的敏感程度，我们首先来简单解释下不同层的敏感程度，不同层的敏感程度主要指的是当某几层分别发生相同程度的变化，导致损失函数变化较大的层就是敏感程度较大的层。现给出结论，通过某一层参数的hessian矩阵的迹就能判断某一层的敏感程度，接下来给出理论分析：</p>
<p>首先假设当前模型存在两个条件：</p>
<ol type="1">
<li><p>模型所对应的损失函数是关于模型参数以及输入二阶可导的，并且模型所对应的损失函数已经收敛至局部最优解，这也意味着，其关于一阶与二阶的导数的相关最优解存在条件已经满足，即一阶导数为0，hessian矩阵为半正定阵。</p></li>
<li><p>由于上文说明hessian矩阵是一个实对称阵，所以<span
class="math inline">\(n\times
n\)</span>的hessian矩阵可以给出n个正交特征向量<span
class="math inline">\(v_{1},v_{2},...,v_{n}\)</span>，对于某一层参数或者某一层的激活值我们求其hessian，假设这一层参数我们加入了一些变化记作<span
class="math inline">\(\Delta
W\)</span>，这个变化我们可以使用本层hessian矩阵产生的正交的特征向量来进行表示，假设这个表示是<span
class="math inline">\(\Delta W = \sum_{i = 1}^{n}\alpha
v_{i}\)</span>。</p></li>
</ol>
<p>基于以上的假设，我们可以给出一些结论：
结论1：如果网络中两层权重或者两层激活值所加的摄动是一样的，即 <span
class="math inline">\(\| \Delta W_{1} \|\_{2}^{2} = \| \Delta W_{2}
\|\_{2}^{2}\)</span>则有当<span
class="math inline">\(\frac{1}{n_{1}}Tr(\nabla^{2}\_{W_{1}}L(W_{1}^{\*}))
&lt; \frac{1}{n_{2}}Tr(\nabla^{2}\_{W_{2}}L(W_{2}^{\*}))\)</span>时<span
class="math inline">\(L(W_{1}^{\*} + \Delta W_{1}^{\*}, W_{2}^{\*}, ...
+ W_{L}^{\*}) &lt; L(W_{1}^{\*}, W_{2}^{\*} + \Delta
W_{2}^{\*},...,W_{L}^{\*})\)</span>，网络权重或激活值变化大小同等程度下，被变化的参数的hessian的矩阵迹越小，变化程度越小。
证明如下： 设<span
class="math inline">\(W_{1}^{\*}\)</span>关于loss函数的hessian矩阵为<span
class="math inline">\(H_{1}\)</span>，其一阶导数设为<span
class="math inline">\(g_{1}\)</span>，那么当<span
class="math inline">\(W_{1}^{\*}\)</span>增加<span
class="math inline">\(\Delta
W_{1}\)</span>使用前文所说的泰勒展开可以得到如下表达： <span
class="math inline">\(L(W_{1}^{\*} + \Delta W_{1}^{\*}) = L(W_{1}^{\*})
+ g^{T}\_{1} \Delta W_{1}^{\*} + \frac{1}{2}\Delta
W_{1}^{\*T}H_{1}\Delta W_{1}^{*} = L(W_{1}^{*}) + \frac{1}{2}\Delta
W_{1}^{\*T}H_{1}\Delta W_{1}^{\*}\)</span></p>
<p>假设此时的网络已经训练收敛，即loss损失函数已经达到局部最小值点，由于此时所在的位置是极值点，那么梯度<span
class="math inline">\(g_{1}\)</span>为0，同时根据前文的推导，这里选择的摄动<span
class="math inline">\(\Delta\)</span>可使用hessian的正交向量进行表示，于是可以的出结论：
<span class="math display">\[\Delta W_{1}^{\*T}H_{1}\Delta W_{1}^{\*} =
\sum_{i = 1}^{n_{1}}\alpha_{1} v_{i}^{1T}H_{1} \alpha_{1} v_{i}^{1} =
\alpha_{1}^{2} \sum_{i = 1}^{n_{1}}\lambda_{i}^{1}\]</span></p>
<p>在这里<span
class="math inline">\(\lambda_{i}^{1}\)</span>表示相应的hessian矩阵的特征值，<span
class="math inline">\(\lambda_{i}^{1}\)</span>的特征向量为<span
class="math inline">\(v_{i}^{1}\)</span>，同样，关于<span
class="math inline">\(W_{2}\)</span>也有相关的计算： <span
class="math display">\[\Delta W_{2}^{\*T}H_{2}\Delta W_{2}^{\*} =
\alpha_{2}^{2}\sum_{i = 1}^{n_{2}}\lambda_{i}^{2}\]</span></p>
<p>因为<span class="math inline">\(W_{1}\)</span>与<span
class="math inline">\(W_{2}\)</span>同属于一个网络，所以计算的loss值中<span
class="math inline">\(L{W_{1}} = L{W_{2}}\)</span>，所以有： <span
class="math display">\[L(W_{1} + \Delta W_{1}) - L(W_{2} + \Delta W_{2})
= L(W_{1}) + \frac{1}{2} \Delta W_{1}^{T}H_{1}\Delta W_{1} - L(W_{2}) -
\frac{1}{2} \Delta W_{2}^{T}H_{1}\Delta W_{2}\]</span> <span
class="math display">\[L(W_{1} + \Delta W_{1}) - L(W_{2} + \Delta W_{2})
=\alpha_{1}^{2}\sum_{i = 1}^{n_{1}}\lambda_{i}^{1}
-  \alpha_{2}^{2}\sum_{i = 1}^{n_{2}}\lambda_{i}^{2}\]</span> 因为<span
class="math inline">\(|W_{1}|\_{2}^{2} =
|W_{2}|\_{2}^{2}\)</span>所以<span
class="math inline">\(\sqrt{n_{1}}\alpha_{1} =
\sqrt{n_{2}}\alpha_{2}\)</span>，带入上式可得：</p>
<p><span class="math display">\[
L(W_{1} + \Delta W_{1}) - L(W_{2} + \Delta W_{2}) =
\alpha_{1}^{2}(\frac{1}{n_{1}}\sum_{i = 1}^{n_{1}}\lambda_{i}^{1} -
\frac{1}{n_{2}}\sum_{i = 1}^{n_{2}}\lambda_{i}^{2})\]</span> $$</p>
<p>两个损失函数的大小与特征值之和也就是hessian的迹有关。</p>
<p>所以可以得出一个结论，在激活值接收的变化程度相同的情况下，激活值的hessian矩阵迹越大，所造成的变化越大。也就是说，迹越大，这一层参数对变化越敏感。</p>
<h4 id="hessian矩阵迹的求解">hessian矩阵迹的求解</h4>
<p>现在分析hessian矩阵迹计算的时间复杂度，假设对W求解hessian矩阵的迹，W是n维向量，则求解hessian矩阵所需时间为<span
class="math inline">\(O(n^{2})\)</span>，求解hessian的特征值有一些方法，如QR分解，Jacobi方法，Hutchinson方法这里简要分析几类方法的时间复杂度:
1.
QR方法主要通过将实对称矩阵分解为正交矩阵和上三角矩阵来求解特征值，具体来说是使用Householder变换和Givens变换逐步的迭代来达到目的，时间复杂度在<span
class="math inline">\(O(n^{3})\)</span>左右 2.
Jacobi方法是一种用于求解实对称矩阵特征值的一种方法，通过一种旋转矩阵J将目标矩阵A变为对角阵D。
<span class="math display">\[
D = (J_{1}^{T} J_{2}^{T}...J_{k}^{T}) A (J_{1}J_{2}...J_{k})
\]</span></p>
<p>现在分析时间复杂度，
由于每次旋转只会影响两行两列数据，采用顺序遍历元素的方法，最少的变换次数为<span
class="math inline">\(O(n_{2})\)</span>次，合并旋转变换矩阵的复杂度为<span
class="math inline">\(O(n)\)</span>，对于表达式<span
class="math inline">\(\|(J_{1}^{T}J_{2}^{T}...J_{k}^{T}) A
(J_{1}J_{2}...J_{k}) - diag{(J_{1}^{T}J_{2}^{T}...J_{k}^{T}) A
(J_{1}J_{2}...J_{k}})\|^{2}\_{F} &lt; \varepsilon\)</span>，<span
class="math inline">\(\varepsilon\)</span>趋近于0，当上述表达式成立的时候，表示A已经被对角化，此时可以使用阈值来判断一些元素是否需要旋转，可以将时间复杂度降下来一点，但是最坏时间复杂度依旧是<span
class="math inline">\(O(n^{3})\)</span></p>
<p>由于网络中参数数量较大，如果将网络中所有相关的参数求一遍hessian矩阵的迹会导致时间非常长，在这里可以使用近似解来代替精确解，这是一种在时间复杂度和解的准确性之间的一种平衡取舍。</p>
<ol start="3" type="1">
<li>Hutchinson迹估计方法</li>
</ol>
<p>Hutchinson迹估计方法使用了标准的蒙特卡洛方法来估计，与之相类似的还有Gaussian迹估计方法以及Rayleight-quotient迹估计方法，唯一的区别是在统计中的随机变量分不是不一样的，这里先给出结论：</p>
<p><span class="math display">\[
E(z^{T}Az) = trace(A)
\]</span> 其中A为<span class="math inline">\(n\times
n\)</span>的方阵，z为n维向量，其元素分布符合<span
class="math inline">\((P(z_{i} = \pm 1) =
\frac{1}{2})\)</span>，可见<span
class="math inline">\(z_{T}Az\)</span>是trace(A)无偏估计量。在Gaussian和Rayleight-quotient估计方法中表达式也类似，但主要区别在于z的分布不一样。下面证明这个结论：
设<span class="math inline">\(z\)</span>为符合分布<span
class="math inline">\((P(z_{i} = \pm 1) =
\frac{1}{2})\)</span>的特征向量，则有： <span class="math display">\[
z = (z_{1},z_{2},...,z_{n})^{T}
\]</span> 因为z中元素分布独立，所以有 <span class="math display">\[
E(z_{i}z{j}) = \frac{1}{4}(1 \times -1) + \frac{1}{4}(-1 \times 1) +
\frac{1}{4}(-1 \times -1) + \frac{1}{4}(1 \times 1) = 0 \\
E(z_{i}z{i}) = \frac{1}{2}(-1 \times -1) + \frac{1}{2}(1 \times 1) = 1
\]</span> 于是有 <span class="math display">\[
E(zz^{T}) = I_{n}
\]</span></p>
<p><span class="math display">\[Tr(A) = Tr(AI) = Tr(AE(zz^{T})) =
Tr(E(Azz^{T})) = E(Tr(Azz^{T}))\]</span></p>
<p>由于两个非零向量相乘所得的矩阵秩为1，所以<span
class="math inline">\(Tr(Azz^{T}) = |Azz^{T}| = |z^{T}Az| =
z^{T}Az\)</span>，所以有：</p>
<p><span class="math display">\[E(Tr(Azz^{T})) = E(z^{T}Az)\]</span>
即<span class="math inline">\(Tr(A) = E(z^{T}Az) = \frac{1}{m} \sum_{i =
1}^{m}z_{i}^{T}Az_{i}\)</span></p>
<p>于是可以使用统计的方法来完成对hessian迹的计算，时间复杂度为O(mn^{2})，其中n表示矩阵的维度，m表示迭代次数，是求解期望过程中使用的参数，时间复杂度远小于原来的情况。</p>
<p><a
href="Randomized%20algorithms%20for%20matrices%20and%20data">参考</a> <a
href="Randomized%20Algorithms%20for%20Estimating%20the%20Trace%20of%20an%20Implicit%20Symmetric%20Positive%20Semi-Definite%20Matrix">参考</a>
0. <a
target="_blank" rel="noopener" href="https://blog.csdn.net/u012347027/article/details/110732488">介绍hawq2的方法</a></p>
<h3
id="hessian矩阵迹计算在代码上的实现">hessian矩阵迹计算在代码上的实现</h3>
<p>有了迹的求法，还剩一个问题就是如何在代码上实现这样的功能。由于需要计算激活值的hessian的迹，但是激活值在网络中计算一遍之后就会消失，同时反向传播的时候数据集的数据张量并不会带有相应的梯度，为了解决这两个问题，需要手动写一个能记录激活值的层，同时还要明白传入的激活值如何能让网络计算激活值的梯度，进而再去计算hessian矩阵的梯度。</p>
<ol type="1">
<li>如何记录计算过程中的激活值</li>
</ol>
<p>一个比较节约事件的方案就是使用原来开发的蒸馏框架，其可以通过修改计算图来实现中间激活值的输出，但是由于框架中蒸馏层需要指定，需要将多各层写在配置文件中，同时如果计算hessian矩阵迹的代码使用了蒸馏中的代码会导致代码耦合，后续如果对蒸馏框架升级，那么hessian矩阵迹计算代码也要进行相应修改，于是本工作采用了两个模块分开设计的思路，重新实现了一个层，包括网络中的卷积层、全连接层等常用的层，每一层中包含一个记录激活指的变量，这样后续就可以使用这个变量，计算关于激活值的hessian的迹。</p>
<ol start="2" type="1">
<li>如何计算hessian矩阵</li>
</ol>
<p>激活值记录之后就可以计算hessian矩阵了，为了能正确的计算hessian矩阵，还需要了解一下pytorch中是如何进行梯度计算的。
pytorch中的tersor在计算的时候是没有办法计算梯度的，例如，输入神经网络的输入本来相对于loss是变量，但是却没有计算梯度，主要原因在于tensor在设置时需要满足如下条件。
1.
tensor的值为浮点数，如果变量类型是定点数，那么如果变量是模型中的参数在反向传播之后更新变量会可能造成定点数变为浮点数，这样会造成一些矛盾。所以规定，所有进行梯度计算的变量类型为浮点数。
2.
tensor中选项<code>requires_grad = True</code>需要设置，因为pytorch为了节约设备的存储空间，如果不设置<code>requires_grad</code>选项，那么变量不会对梯度进行记录，加上这个设置之后，后续由其计算出的所有tensor都会是<code>requires_grad = False</code></p>
<p>所以为了求解hessian矩阵的迹，首先对于输入数据集的tensor应加上<code>requires_grad = True</code>，方便后续求解梯度。pytorch在做相关运算的时候会将得到这个变量之前使用的函数记录下来，当计算的最终结果调用了backward这个函数之后，就会根据这个函数距离去逐步调用相应的backward函数，pytorch为每一个函数都写好了导数函数，利用链式法则就可将当前变量的梯度求解出来，例如<span
class="math inline">\(y =
e^{x}\)</span>这个函数就有一个backward函数，其设置梯度为<span
class="math inline">\(e_{x}\)</span>，并与输入x的梯度相乘得到最终链式法则应有的梯度。对于一些卷积层等只需要调用相应的乘法等基本函数的梯度函数即可。</p>
<p>当然上述过程解释的是backward的计算过程，这只是一阶导数，在本工作中需要得到激活值的hessian，所以还需要进行二阶导数的计算，进行这种计算依旧要使用记录在tensor中上一步使用的函数<code>grad_fn</code>这个信息，但是无法再利用backward再次计算二阶导数，应使用torch.autograd.grad()函数，torch.autograd.grad()可以根据函数计算结果和输入变量计算最终的梯度矩阵，如果函数计算结果是一个梯度那么所求的的矩阵就是hessain矩阵。所以模型的hessian矩阵可以使用如下流程进行处理：
0. 输入张量设置<code>requires_grad = True</code> 1.
利用torch.autograd.grad()求解每一层的激活值对loss的一阶导数grads 2.
利用torch.autograd.grad()求解每层的激活值对本层激活值关于loss的一阶导数的梯度，记为hessians</p>
<ol start="3" type="1">
<li>如何进行hessian矩阵的迹的求解</li>
</ol>
<p>观察表达式</p>
<p><span class="math display">\[
Tr(A) = E(z^{T}Az) = \frac{1}{m} \sum_{i = 1}^{m}z_{i}^{T}Az_{i}
\]</span></p>
<p>发现这是个需要循环处理的函数，我们给出退出循环的两个条件 1.
上一次循环处理的结果和本次的结果之间差距小于T <span
class="math display">\[
Tr(A)\_{m} = \frac{1}{m} \sum_{i = 1}^{m}z_{i}^{T}Az_{i} \\
Tr(A)\_{m + 1} - Tr(A)\_{m} &lt; T
\]</span> 2. 循环次数已经达到上限M</p>
<p>表达式中的<span
class="math inline">\(z_{i}\)</span>可以直接使用二项分布给出相关的张量，然后将二项分布中的0改为-1，完成了分布的替换。</p>
<h3
id="如何利用hessian矩阵的迹进行蒸馏层的选择">如何利用hessian矩阵的迹进行蒸馏层的选择</h3>
<p>网络中中间层的结果就是相应网络学习到的特征，有些特征比较重要，有些特征稍微不那么重要，我们可以通过改变特征之后对网络结果的影响来判断。前面分析到，特征的hessain矩阵迹可以判断特征对损失函数大小的影响，所以可以通过计算所有层特征的hessian矩阵，使用Hutchinson统计方法统计hessian矩阵的迹，那么迹最大的几层就是特征最重要的层，所以在使用特征蒸馏方案的时候，就选择迹最大的几层，输出出来之后在配置文件中设置。</p>
<h3 id="损失函数的设置">损失函数的设置</h3>
<p>由于每一个蒸馏的特征层有不同的重要程度，所以在设置损失函数中蒸馏的损失时，其损失对应的权重应随着特征的重要程度的增加而增加。</p>
<h1 id="结果分析">结果分析</h1>
<ol type="1">
<li>lambda 设置多个</li>
<li></li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://skk1faker.github.io/2023/10/22/miniob-aggregation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/acm-icpc.png">
      <meta itemprop="name" content="skk1faker">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="skk1faker 笔记">
      <meta itemprop="description" content="登峰造极">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | skk1faker 笔记">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/10/22/miniob-aggregation/" class="post-title-link" itemprop="url">miniob_aggregation</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-10-22 15:16:48" itemprop="dateCreated datePublished" datetime="2023-10-22T15:16:48+08:00">2023-10-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-02-22 12:04:39" itemprop="dateModified" datetime="2025-02-22T12:04:39+08:00">2025-02-22</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>待办 - [ ] lex和yacc是如何联系起来的
RETURN_TOKEN中的宏就是在yacc中的标识，lex识别到字符串，返回给yacc一个宏,yacc对这些符号定义终结符和非终结符，并且一些终结符和非终结符都有相应的值
- [ ] 完善stmt中接收count参数的问题 注意这个错误 <img src="wocao.png"
alt="wocao" /> <img
src="miniob-aggregation/_注意列遍历要倒过来，不然会出现问题.png"
alt="_注意列遍历要倒过来，不然会出现问题" /> <img
src="miniob-aggregation/_生成stmt的时候列遍历要倒过来.png"
alt="_生成stmt的时候列遍历要倒过来" /></p>
<h2 id="mutil-index">mutil-index</h2>
<h3 id="分享一个接口">分享一个接口</h3>
<ol type="1">
<li>memmove(__item_at(index + 1), __item_at(index),
(static_cast<size_t>(size()) - index) * item_size()); //skt1faker:
用于有重叠字符串之间的复制</li>
<li>std::advance(iter,num); // skt1faker
iter必须是一个迭代器，向前移动num或者倒退（-num）个单位。</li>
<li>vector.swap和vector.assign的区别
swap之后的,原vector中的元素全部消失，assign之后，原vector中的元素依旧存在</li>
</ol>
<p>assign <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vector&lt;int&gt; temlist;</span><br><span class="line">temlist.assign(list.begin(), list.end());</span><br></pre></td></tr></table></figure> 一样的复制了一份数据，list中的数据不变。</p>
<p>swap <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vector&lt;int&gt; temlist;</span><br><span class="line">temlist.swap(list);</span><br></pre></td></tr></table></figure>
将list中数据全部移到temlist中，此时list中为空了</p>
<ul>
<li>需要修改一下比较还有初始化的逻辑。</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://skk1faker.github.io/2023/10/17/miniob-date/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/acm-icpc.png">
      <meta itemprop="name" content="skk1faker">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="skk1faker 笔记">
      <meta itemprop="description" content="登峰造极">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | skk1faker 笔记">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/10/17/miniob-date/" class="post-title-link" itemprop="url">miniob_date</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-10-17 13:10:59" itemprop="dateCreated datePublished" datetime="2023-10-17T13:10:59+08:00">2023-10-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-02-22 12:04:39" itemprop="dateModified" datetime="2025-02-22T12:04:39+08:00">2025-02-22</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://skk1faker.github.io/2023/10/17/2Dhash/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/acm-icpc.png">
      <meta itemprop="name" content="skk1faker">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="skk1faker 笔记">
      <meta itemprop="description" content="登峰造极">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | skk1faker 笔记">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/10/17/2Dhash/" class="post-title-link" itemprop="url">二维hash</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-10-17 09:39:45" itemprop="dateCreated datePublished" datetime="2023-10-17T09:39:45+08:00">2023-10-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-02-22 12:04:39" itemprop="dateModified" datetime="2025-02-22T12:04:39+08:00">2025-02-22</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="二维hash表达式">二维hash表达式</h1>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*================================================================</span></span><br><span class="line"><span class="comment">*   Copyright (C) 2023 Wangxinpeng. All rights reserved.</span></span><br><span class="line"><span class="comment">*</span></span><br><span class="line"><span class="comment">*   filename：    uva11019.cpp</span></span><br><span class="line"><span class="comment">*   username:     skt1faker</span></span><br><span class="line"><span class="comment">*   create time:  19:27  2023.10.16</span></span><br><span class="line"><span class="comment">    email:        skk1faker@163.com</span></span><br><span class="line"><span class="comment">*   descripe:</span></span><br><span class="line"><span class="comment">*</span></span><br><span class="line"><span class="comment">================================================================*/</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="meta">#<span class="keyword">define</span> ll long long</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> ull unsigned long long </span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> DEBUG0</span></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> mod = (<span class="type">int</span>)<span class="number">1e9</span> + <span class="number">7</span>;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> mod1 = <span class="number">131</span>;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> mod2 = <span class="number">13561</span>;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> maxx = (<span class="type">int</span>)<span class="number">1e3</span> + <span class="number">10</span>;</span><br><span class="line">ull hash_s[maxx][maxx]; <span class="comment">// 二维hash</span></span><br><span class="line">ull hash_f[maxx][maxx]; <span class="comment">// 模版的hash</span></span><br><span class="line"><span class="type">char</span> s[maxx][maxx],f[maxx][maxx];</span><br><span class="line">ull powi[maxx];</span><br><span class="line">ull powj[maxx];</span><br><span class="line"></span><br><span class="line"><span class="function">ull <span class="title">get_hash</span><span class="params">(<span class="type">int</span> x1,<span class="type">int</span> y1,<span class="type">int</span> x2,<span class="type">int</span> y2)</span></span>&#123;</span><br><span class="line">  <span class="keyword">return</span> hash_s[x2][y2] - (hash_s[x1 - <span class="number">1</span>][y2]) * powi[x2 - x1 + <span class="number">1</span>] - hash_s[x2][y1 - <span class="number">1</span>] * powj[y2 - y1 + <span class="number">1</span>] + hash_s[x1 - <span class="number">1</span>][y1 - <span class="number">1</span>] * powi[x2 - x1 + <span class="number">1</span>] * powj[y2 - y1 + <span class="number">1</span>];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">init</span><span class="params">(<span class="type">int</span> n)</span></span>&#123;</span><br><span class="line">  powi[<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line">  powj[<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">1</span>;i&lt;=n;i++)&#123;</span><br><span class="line">    powi[i] = powi[i - <span class="number">1</span>] * mod1;</span><br><span class="line">    powj[i] = powj[i - <span class="number">1</span>] * mod2;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">init_hash</span><span class="params">(<span class="type">int</span> n,<span class="type">int</span> m,ull hash_list[][maxx],<span class="type">char</span> ss[][maxx])</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">1</span>;i &lt;= n;i++)&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> j = <span class="number">1</span>;j &lt;= m;j++)&#123;</span><br><span class="line">      hash_list[i][j] = hash_list[i][j - <span class="number">1</span>] * powj[<span class="number">1</span>] + hash_list[i - <span class="number">1</span>][j] * powi[<span class="number">1</span>] + (ss[i][j] - <span class="string">&#x27;a&#x27;</span>) - hash_list[i - <span class="number">1</span>][j - <span class="number">1</span>] * powi[<span class="number">1</span>] * powj[<span class="number">1</span>];</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="type">int</span> T;</span><br><span class="line">  cin&gt;&gt;T;</span><br><span class="line">  <span class="built_in">init</span>(maxx);</span><br><span class="line">  <span class="keyword">while</span>(T--)&#123;</span><br><span class="line">    <span class="type">int</span> n[<span class="number">2</span>],m[<span class="number">2</span>];</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">&quot;%d%d&quot;</span>,&amp;n[<span class="number">0</span>],&amp;m[<span class="number">0</span>]);</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">1</span>;i &lt;= n[<span class="number">0</span>];i++)&#123;</span><br><span class="line">      <span class="built_in">scanf</span>(<span class="string">&quot;%s&quot;</span>,s[i] + <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">init_hash</span>(n[<span class="number">0</span>],m[<span class="number">0</span>],hash_s,s);</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">&quot;%d%d&quot;</span>,&amp;n[<span class="number">1</span>],&amp;m[<span class="number">1</span>]);</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">1</span>;i &lt;= n[<span class="number">1</span>];i++)&#123;</span><br><span class="line">      <span class="built_in">scanf</span>(<span class="string">&quot;%s&quot;</span>,f[i] + <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">init_hash</span>(n[<span class="number">1</span>],m[<span class="number">1</span>],hash_f,f);</span><br><span class="line">    <span class="type">int</span> ans = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = n[<span class="number">1</span>];i &lt;= n[<span class="number">0</span>];i++)&#123;</span><br><span class="line">      <span class="keyword">for</span>(<span class="type">int</span> j = m[<span class="number">1</span>];j &lt;= m[<span class="number">0</span>];j++)&#123;</span><br><span class="line">        <span class="keyword">if</span>(hash_f[n[<span class="number">1</span>]][m[<span class="number">1</span>]] == <span class="built_in">get_hash</span>(i - n[<span class="number">1</span>] + <span class="number">1</span>, j - m[<span class="number">1</span>] + <span class="number">1</span>,i,j))&#123;</span><br><span class="line">          ans++;</span><br><span class="line">          <span class="comment">//cout&lt;&lt;i - n[1] + 1&lt;&lt;&#x27; &#x27;&lt;&lt;j - m[1] + 1&lt;&lt;endl;</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    cout&lt;&lt;ans&lt;&lt;endl;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*TLE</span></span><br><span class="line"><span class="comment">const int maxx = (int)1e3 + 10;</span></span><br><span class="line"><span class="comment">ll sums[maxx][maxx][2];</span></span><br><span class="line"><span class="comment">ll sums_template[maxx][maxx][2];</span></span><br><span class="line"><span class="comment">char val[maxx][maxx];</span></span><br><span class="line"><span class="comment">char template_val[maxx][maxx];</span></span><br><span class="line"><span class="comment">ll pw[maxx][2];</span></span><br><span class="line"><span class="comment">ll mod[2];</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">ll qpow(ll a, ll b, int mode) &#123;</span></span><br><span class="line"><span class="comment">  if(pw[b][mode] != 0)</span></span><br><span class="line"><span class="comment">    return pw[b][mode];</span></span><br><span class="line"><span class="comment">  ll m = mod[mode];</span></span><br><span class="line"><span class="comment">  ll ans = 1;</span></span><br><span class="line"><span class="comment">  ll temp = a;</span></span><br><span class="line"><span class="comment">  while (b) &#123;</span></span><br><span class="line"><span class="comment">    if (b &amp; 1) &#123;</span></span><br><span class="line"><span class="comment">      ans *= temp;</span></span><br><span class="line"><span class="comment">      ans %= m;</span></span><br><span class="line"><span class="comment">    &#125;</span></span><br><span class="line"><span class="comment">    temp *= temp;</span></span><br><span class="line"><span class="comment">    temp %= m;</span></span><br><span class="line"><span class="comment">    b /= 2;</span></span><br><span class="line"><span class="comment">  &#125;</span></span><br><span class="line"><span class="comment">  return pw[b][mode] = ans;</span></span><br><span class="line"><span class="comment">&#125;</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">ll get_val(int x1, int y1, int x2, int y2, int m, int mode) &#123;</span></span><br><span class="line"><span class="comment">  return (sums[x1][y1][mode] * qpow(26, (x2 - x1) * m + (y2 - y1), mode)) %</span></span><br><span class="line"><span class="comment">         mod[mode];</span></span><br><span class="line"><span class="comment">&#125;</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">ll get_hash(int x1, int y1, int x2, int y2, int m, int mode) &#123; // n表示列数</span></span><br><span class="line"><span class="comment">  ll ans = (sums[x2][y2][mode] + get_val(x1 - 1, y1 - 1, x2, y2, m, mode) </span></span><br><span class="line"><span class="comment">            - get_val(x1 - 1, y2, x2, y2, m, mode) </span></span><br><span class="line"><span class="comment">            - get_val(x2, y1 - 1, x2, y2, m, mode) + 2 * mod[mode]) % mod[mode];</span></span><br><span class="line"><span class="comment">  return ans;</span></span><br><span class="line"><span class="comment">&#125;</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">void init(int n, int m,int length,char s[][maxx],ll sum[][maxx][2]) &#123;</span></span><br><span class="line"><span class="comment">  for (int i = 1; i &lt;= n; i++) &#123;</span></span><br><span class="line"><span class="comment">    ll temp_sum[2] = &#123;0&#125;;</span></span><br><span class="line"><span class="comment">    for (int j = 1; j &lt;= m; j++) &#123;</span></span><br><span class="line"><span class="comment">      for (int k = 0; k &lt; 2; k++) &#123;</span></span><br><span class="line"><span class="comment">        temp_sum[k] *= 26;</span></span><br><span class="line"><span class="comment">        temp_sum[k] += s[i][j] - &#x27;a&#x27;;</span></span><br><span class="line"><span class="comment">        temp_sum[k] %= mod[k];</span></span><br><span class="line"><span class="comment">        sum[i][j][k] = (sum[i - 1][j][k] * qpow(26 ,length,k) + temp_sum[k]) % mod[k];</span></span><br><span class="line"><span class="comment">      &#125;</span></span><br><span class="line"><span class="comment">    &#125;</span></span><br><span class="line"><span class="comment">  &#125;</span></span><br><span class="line"><span class="comment">&#125;</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">int main() &#123; </span></span><br><span class="line"><span class="comment">  int T;</span></span><br><span class="line"><span class="comment">  cin&gt;&gt;T;</span></span><br><span class="line"><span class="comment">  mod[0] = mod1;</span></span><br><span class="line"><span class="comment">  mod[1] = mod2;</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">  while(T--)&#123;</span></span><br><span class="line"><span class="comment">    int n,m;</span></span><br><span class="line"><span class="comment">    scanf(&quot;%d%d&quot;,&amp;n,&amp;m);</span></span><br><span class="line"><span class="comment">    for(int i = 1;i &lt;= n;i++)&#123;</span></span><br><span class="line"><span class="comment">      scanf(&quot;%s&quot;,val[i] + 1);</span></span><br><span class="line"><span class="comment">    &#125;</span></span><br><span class="line"><span class="comment">    init(n,m,m,val,sums);</span></span><br><span class="line"><span class="comment">    int n1,m1;</span></span><br><span class="line"><span class="comment">    cin&gt;&gt;n1&gt;&gt;m1;</span></span><br><span class="line"><span class="comment">    for(int i = 1;i &lt;= n1;i++)&#123;</span></span><br><span class="line"><span class="comment">      scanf(&quot;%s&quot;,template_val[i] + 1);</span></span><br><span class="line"><span class="comment">    &#125;</span></span><br><span class="line"><span class="comment">    if(n1 &gt; n || m1 &gt; m)&#123;</span></span><br><span class="line"><span class="comment">      cout&lt;&lt;0&lt;&lt;endl;</span></span><br><span class="line"><span class="comment">      continue;</span></span><br><span class="line"><span class="comment">    &#125;</span></span><br><span class="line"><span class="comment">    init(n1,m1,m,template_val,sums_template);</span></span><br><span class="line"><span class="comment">    ll template_hash_val[2] = &#123;sums_template[n1][m1][0] ,sums_template[n1][m1][1]&#125;;</span></span><br><span class="line"><span class="comment">    int ans = 0;</span></span><br><span class="line"><span class="comment">    for(int i = n1;i &lt;= n;i++)&#123;</span></span><br><span class="line"><span class="comment">      for(int j = m1;j &lt;= m;j++)&#123;</span></span><br><span class="line"><span class="comment">        int pre_i = i - n1 + 1, pre_j = j - m1 + 1;</span></span><br><span class="line"><span class="comment">        int flag = 1;</span></span><br><span class="line"><span class="comment">        for(int k = 0;k &lt; 2;k++)&#123;</span></span><br><span class="line"><span class="comment">          if(template_hash_val[k] != get_hash(pre_i,pre_j,i,j,m,k))flag = 0;</span></span><br><span class="line"><span class="comment">        &#125;</span></span><br><span class="line"><span class="comment">        ans += flag;</span></span><br><span class="line"><span class="comment">      &#125;</span></span><br><span class="line"><span class="comment">    &#125;</span></span><br><span class="line"><span class="comment">    cout&lt;&lt;ans&lt;&lt;endl;</span></span><br><span class="line"><span class="comment">  &#125;</span></span><br><span class="line"><span class="comment">  return 0; </span></span><br><span class="line"><span class="comment">&#125;</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://skk1faker.github.io/2023/10/13/clean-cachefile/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/acm-icpc.png">
      <meta itemprop="name" content="skk1faker">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="skk1faker 笔记">
      <meta itemprop="description" content="登峰造极">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | skk1faker 笔记">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/10/13/clean-cachefile/" class="post-title-link" itemprop="url">清理linux垃圾内容</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-10-13 10:19:41" itemprop="dateCreated datePublished" datetime="2023-10-13T10:19:41+08:00">2023-10-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-02-22 12:41:23" itemprop="dateModified" datetime="2025-02-22T12:41:23+08:00">2025-02-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/linux/" itemprop="url" rel="index"><span itemprop="name">linux</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="一些文件夹">一些文件夹</h1>
<ol type="1">
<li>~/.cache/vscode-cpptools/ipch ipch文件夹是Intelli
Sense（好像是预编译头文件之类的）这个东西产生的缓存文件，占用空间很大！！！每编译一次文件就会产生ipch里对应的一个文件夹。我看了我的ipch文件夹产生了121个对应的文件夹，总共产生了6G多。。。惊人啊！</li>
</ol>
<p>在VSCode找到设置→扩展→C/C++→Intelli Sense Cache
Path，如上图。1处写明了vscode默认Intelli
Sense缓存文件路径是在C盘的，2处可以更改为你要保存缓存文件的位置。据我所知，这些缓存文件删了也不影响的之前的文件的，所以可以随时删~只是每次编译又会重新产生，所以还是把缓存路径改为别的盘吧
<a
target="_blank" rel="noopener" href="https://blog.csdn.net/qq_41688558/article/details/99085069">参考</a></p>
<ol start="2" type="1">
<li>~/.config/Code/Service Worker/CacheStorage
ubuntu系统下，我们的空间会越来越小，有时候不知道大文件藏在哪。其中有一项就是vscode的缓存，时间久了可能会有几十G的空间，把缓存删掉，可以腾出很大空间。
不同电脑的vscode安装目录可能有区别： cd
~/.config/Code/User/workspaceStorage/ rm ./<em> cd
~/.config/Code/Service Worker/CacheStorage$ rm ./</em></li>
</ol>
<p><a
target="_blank" rel="noopener" href="https://blog.csdn.net/u011754972/article/details/120764945">参考</a></p>
<ol start="3" type="1">
<li>anaconda/pkts conda clean
命令是对所有anaconda下的包进行搜索，不用再进入其他环境重复操作 conda
clean -p
这个命令会检查哪些包没有在包缓存中被硬依赖到其他地方，并删除它们 conda
clean -t
刚刚清理的缓存的packages，现在继续清理缓存的压缩包文件，代码如下：conda
clean -t</li>
</ol>
<p>https://blog.csdn.net/zhouchen1998/article/details/124397874
https://blog.csdn.net/Robin_Pi/article/details/115004870</p>
<ol start="4" type="1">
<li>~/.local/share/Trash</li>
</ol>
<p>linux的垃圾箱</p>
<ol start="5" type="1">
<li><p>baobab可以查询大文件，然后进行删除。</p></li>
<li><p>使用snap清理时出现“Save data of snap “docker“ in automatic
snapshot set #3”这说明snap在清理时创建了快照</p></li>
</ol>
<p>如果中途你暂停了，那么可能再次删除会出现 “错误：snap "steam" has
"remove-snap" change in progress”，此时需要简单修复以下：
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">snap changes</span><br><span class="line"><span class="built_in">sudo</span> snap abort &lt;ID number&gt;				<span class="comment">#注意这个id是上一步snap change给出的结果</span></span><br><span class="line"><span class="built_in">sudo</span> reboot</span><br><span class="line">snap <span class="built_in">set</span> core snapshots.automatic.retention=no </span><br><span class="line"><span class="comment"># sudo snap remove steam</span></span><br></pre></td></tr></table></figure></p>
<p>这是只需要使用<code>snap remove --purge &lt;package&gt;</code>即可，</p>
<ol start="7" type="1">
<li>清理snap快照</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">snap saved		<span class="comment">#查看有哪些快照</span></span><br><span class="line">snap forget &lt;<span class="built_in">id</span>&gt; <span class="comment"># id为上一命令输出的滴一列值</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>如果snap
saved中没有快照，那么可以去<code>/var/lib/snapd/snapshots</code>中查看，查到有东西可以直接删除</p>
<p><a
target="_blank" rel="noopener" href="https://askubuntu.com/questions/1283423/is-it-safe-to-delete-var-lib-snapd-snapshots">参考自</a></p>
<ol start="8" type="1">
<li><p>清理snap以前本版本的安装包： 主要使用以下脚本：
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash  </span></span><br><span class="line"><span class="comment">#Removes old revisions of snaps  </span></span><br><span class="line"><span class="comment">#CLOSE ALL SNAPS BEFORE RUNNING THIS  </span></span><br><span class="line"><span class="built_in">set</span> -eu  </span><br><span class="line">LANG=en_US.UTF-8 </span><br><span class="line">snap list --all | awk <span class="string">&#x27;/disabled/&#123;print $1, $3&#125;&#x27;</span> |</span><br><span class="line">	<span class="keyword">while</span> <span class="built_in">read</span> snapname revision; <span class="keyword">do</span></span><br><span class="line">		snap remove <span class="string">&quot;<span class="variable">$snapname</span>&quot;</span> --revision=<span class="string">&quot;<span class="variable">$revision</span>&quot;</span></span><br><span class="line">	<span class="keyword">done</span></span><br></pre></td></tr></table></figure></p></li>
<li><p>软件清理缓存：</p></li>
</ol>
<p>sudo apt install bleachbit <a
target="_blank" rel="noopener" href="https://www.debugpoint.com/4-simple-steps-clean-ubuntu-system-linux/">ubuntu清理的4中方法</a>
https://www.debugpoint.com/4-simple-steps-clean-ubuntu-system-linux/
sudo apt install bleachbit</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://skk1faker.github.io/2023/10/10/graduate/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/acm-icpc.png">
      <meta itemprop="name" content="skk1faker">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="skk1faker 笔记">
      <meta itemprop="description" content="登峰造极">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | skk1faker 笔记">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/10/10/graduate/" class="post-title-link" itemprop="url">graduate</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-10-10 13:19:28" itemprop="dateCreated datePublished" datetime="2023-10-10T13:19:28+08:00">2023-10-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-02-22 12:04:39" itemprop="dateModified" datetime="2025-02-22T12:04:39+08:00">2025-02-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%AF%95%E4%B8%9A/" itemprop="url" rel="index"><span itemprop="name">毕业</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="abandon_the_bayer....">Abandon_the_Bayer....</h2>
<h3 id="暗光处理主要思路">暗光处理主要思路</h3>
<ol type="1">
<li><p>使用两类网络，一类用语将rggb图像变为单一颜色灰度图像，主要是为了统一输入，因为有些输入图像格式为ryyb，这种格式得到的图片质量较高</p></li>
<li><p>为了不让原始的彩色信息丢失，这里使用一个channel-wise的层，他可以接收单一颜色的灰度图片，也接受rggb颜色的图片，然后进行处理得到最终的结果
<img src="pipeline.png" /></p></li>
</ol>
<h2 id="数据集">数据集</h2>
<p>sid数据集介绍(https://blog.csdn.net/tywwwww/article/details/131072339)</p>
<h2 id="量化框架书写">量化框架书写</h2>
<ul>
<li>torch.fx使用介绍
<ul>
<li>计算图相关</li>
</ul></li>
<li>torch.nn.qat的作用
<ul>
<li>量化相关</li>
</ul></li>
<li>mqbench相关<a
target="_blank" rel="noopener" href="https://mqbench.readthedocs.io/en/latest/user_guide/internal/learn_config.html">文档</a>
<ul>
<li>量化框架中层替换分别使用了pytorch中nn.qat库中的一些卷积层，以及mqbench.nn.qat、mqbench.nn.intrinsic.qat
==&gt; 主要为融合relu，batchnormal等层结构</li>
</ul></li>
<li>相关量化方法，mqbench中已经实现的部分 <a
target="_blank" rel="noopener" href="https://mqbench.readthedocs.io/en/latest/get_started/support_matrix.html">内部含有相关论文</a></li>
</ul>
<h2 id="知识蒸馏">知识蒸馏</h2>
<ul>
<li>同样适用torch.fx框架书写蒸馏部分</li>
<li>完成teacher蒸馏框架 <a
target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/353472061">特征蒸馏方案</a></li>
</ul>
<h3 id="中间层的选择">中间层的选择</h3>
<p>选择中间层进行知识蒸馏需要考虑几个因素，包括模型的架构、任务的复杂性和计算资源。以下是一些选择中间层的指导原则：</p>
<ol type="1">
<li><p>特征的重要性：首先，需要分析模型中不同层的特征表示对于任务的重要性。通常来说，越接近模型输出的层包含的特征更加抽象和高级，而越靠近输入的层包含的特征更加原始和低级。根据任务的需要，选择与任务相关性较高的中间层进行蒸馏。</p></li>
<li><p>模型大小和计算资源：蒸馏中间层会增加计算开销，因为要传递更多的特征表示。如果计算资源有限，你可能需要选择较少的中间层进行蒸馏，或者考虑对中间层进行进一步的降维或压缩。</p></li>
<li><p>蒸馏目标：确定蒸馏的目标也很重要。如果你的目标是在小型模型中保留大型模型的全部知识，那么选择更多的中间层进行蒸馏可能是有益的。如果只关注某些方面的知识，可以选择与目标知识相关的中间层。</p></li>
<li><p>实验和调整：最好的方法是进行一些实验和调整，以确定最佳的中间层选择。可以尝试不同的组合，并使用验证集来评估性能，然后选择性能最好的模型。</p></li>
</ol>
<p>总之，选择中间层需要综合考虑任务需求、计算资源和性能指标。根据你的具体情况和目标，可以灵活地调整中间层的选择来进行知识蒸馏。</p>
<h3
id="使用hession矩阵的迹去判断一个层是否需要进行蒸馏">使用hession矩阵的迹去判断一个层是否需要进行蒸馏</h3>
<ol type="1">
<li>hession能判断出一个activation的改变是否能对整个网络影响很大，对于影响很大的层我们认为这是关键输出，也就是特征比较强的层.</li>
</ol>
<h2 id="存在问题">存在问题</h2>
<ul>
<li>downshuffle中的输入没有办法量化 <img src="输入未量化.png"
alt="输入未量化" /></li>
</ul>
<h2 id="正在做的实验以及相关任务">正在做的实验以及相关任务：</h2>
<ul class="task-list">
<li><p><label><input type="checkbox"
checked="" />增加了量化方法修改的功能（不是量化开发板的修改）</label></p></li>
<li><p><label><input type="checkbox"
checked="" />修改结果保存部分的代码</label></p>
<ul class="task-list">
<li><label><input type="checkbox"
checked="" />能够完成最优结果的保存</label></li>
<li><label><input type="checkbox"
checked="" />最优结果对应的检测结果</label></li>
<li><label><input type="checkbox" checked="" />当前模型结果</label></li>
<li><label><input type="checkbox"
checked="" />当前模型保存（模型名含模型对应的袋鼠）</label></li>
<li><label><input type="checkbox"
checked="" />每代检测结果txt文件</label></li>
<li><label><input type="checkbox"
checked="" />完善量化方案配置（dsq，或者还是其他方案）</label></li>
</ul></li>
<li><p><label><input type="checkbox"
checked="" />启动对照实验</label></p>
<ul class="task-list">
<li><label><input type="checkbox"
checked="" />启动多个其他方案的量化</label>
<ul class="task-list">
<li><label><input type="checkbox" checked="" />dsq方法</label></li>
</ul></li>
<li><label><input type="checkbox"
checked="" />启动一个本方案的量化</label>
<ul class="task-list">
<li><label><input type="checkbox" checked="" />多种蒸馏权重</label></li>
<li><label><input type="checkbox" />不同位置的蒸馏层</label>
<ul class="task-list">
<li><label><input type="checkbox" checked="" />底部</label></li>
<li><label><input type="checkbox" />中部</label></li>
<li><label><input type="checkbox" />开头</label></li>
</ul></li>
<li><label><input type="checkbox" />不同位置的量化效果(不加蒸馏)</label>
<ul class="task-list">
<li><label><input type="checkbox" />开头结尾</label></li>
<li><label><input type="checkbox" />中间位置</label></li>
</ul></li>
</ul></li>
</ul></li>
<li><p><label><input type="checkbox"
checked="" />未加量化和蒸馏的代码泡一下 ## 论文章节</label></p></li>
<li><p><label><input
type="checkbox" />量化蒸馏与暗光增强的介绍</label></p>
<ul class="task-list">
<li><label><input type="checkbox" checked="" />量化</label>
<ul class="task-list">
<li><label><input type="checkbox"
checked="" />多种量化方式的数学原理</label></li>
<li><label><input type="checkbox"
checked="" />量化训练的几种方案，以及量化的难点</label></li>
<li><label><input type="checkbox" checked="" />量化如何进行训练</label>
<ul class="task-list">
<li><label><input type="checkbox"
checked="" />为什么需要统计参数</label></li>
<li><label><input type="checkbox"
checked="" />训练怎么才能达到最好的效果（展示未使用pytorch.fx量化框架加载模型的结果）</label></li>
</ul></li>
</ul></li>
<li><label><input type="checkbox" />暗光增强</label>
<ul class="task-list">
<li><label><input type="checkbox" />暗光网络</label></li>
<li><label><input type="checkbox" />暗光的难点</label></li>
</ul></li>
<li><label><input type="checkbox" />蒸馏</label>
<ul class="task-list">
<li><label><input type="checkbox" />蒸馏的介绍</label></li>
<li><label><input type="checkbox" />为什么引入蒸馏</label></li>
</ul></li>
</ul></li>
<li><p><label><input type="checkbox" />量化蒸馏框架的介绍</label></p>
<ul class="task-list">
<li><label><input type="checkbox" />量化框架的主要技术</label></li>
<li><label><input type="checkbox" />层融合原因</label>
<ul class="task-list">
<li><label><input
type="checkbox" />batchnormal、relu为什么需要需要融合</label></li>
<li><label><input
type="checkbox" />融合应该是一个什么样的计算方式</label></li>
</ul></li>
<li><label><input type="checkbox" />如何引入蒸馏</label></li>
<li><label><input type="checkbox" />量化方法的选择</label></li>
<li><label><input
type="checkbox" />框架的方便性与广泛适用性演示（可以进通过几行代码来完成量化训练的步数）</label>
<ul class="task-list">
<li><label><input type="checkbox" />量化层插入展示</label></li>
<li><label><input type="checkbox" />量化配置</label></li>
<li><label><input type="checkbox" />框架使用使用</label></li>
</ul></li>
</ul></li>
<li><p><label><input
type="checkbox" />暗光网络的量化蒸馏方案</label></p>
<ul class="task-list">
<li><label><input type="checkbox" />如何加入蒸馏，为什么要这样加</label>
<ul>
<li><label><input type="checkbox" />蒸馏层如何进行确定</label></li>
<li><label><input
type="checkbox" />activation的hession矩阵迹计算原理</label></li>
<li>[ ]</li>
</ul></li>
<li><label><input type="checkbox" />怎样实现量化</label>
<ul class="task-list">
<li><label><input
type="checkbox" />位数为什么是前面和后面有所扩大</label></li>
<li><label><input type="checkbox" />梯度反向传播方法方案</label></li>
</ul></li>
</ul></li>
<li><p><label><input type="checkbox" />实验结果展示</label></p>
<ul class="task-list">
<li><label><input type="checkbox" />对比实验结果</label></li>
</ul></li>
</ul>
<h3 id="sun-oct-15-171616-hkt-2023">Sun Oct 15 17:16:16 HKT 2023</h3>
<p><del>1. 开启两个量化 + 蒸馏的实验 ,
只有最后一层前一层的输出被蒸馏，使用了两个系数，一个是0.1，另一个是0.5（0.1的时间比较早，0.5是后放的）</del>
<del>2.
重新开始实验，发现自己的量化统计方式有些问题，没有加入训练元素（可能实际上也不需要加上训练元素，因为加载的模型早就稳定了）</del>
3.
突然发现模型中量化模型没有加载原始模型，所以效果很差，所以这里开了第三个模型</p>
<h3 id="mon-oct-16-173948-hkt-2023">Mon Oct 16 17:39:48 HKT 2023</h3>
<ol type="1">
<li>昨天起的实验没有成功（因为服务器的一些问题导致一直开在数据加载阶段），在这里加入了一些特殊层数量化的方案，将两段网络的输入和输出的都进行高位数的量化，同时将两段网络中结尾上一层的输出作为蒸馏的依据,配置如下，与以往的实验相比，效果已经好起来了。
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">quant :</span> <span class="literal">True</span></span><br><span class="line"><span class="attr">test :</span> <span class="literal">False</span></span><br><span class="line"><span class="attr">onnx :</span> <span class="literal">False</span></span><br><span class="line"><span class="attr">quant_teacher :</span> <span class="literal">True</span></span><br><span class="line"><span class="attr">gpu_id :</span> <span class="number">3</span></span><br><span class="line"><span class="attr">batch_size :</span> <span class="number">6</span></span><br><span class="line"><span class="attr">debug :</span> <span class="literal">False</span></span><br><span class="line"><span class="attr">pdb_debug :</span> <span class="literal">False</span></span><br><span class="line"><span class="attr">load_model :</span> <span class="string">./onnx/SID_weights_690000.pth</span></span><br><span class="line"><span class="attr">load_quant_model :</span> </span><br><span class="line"><span class="attr">load_teacher_model :</span> <span class="string">./onnx/SID_weights_690000.pth</span></span><br><span class="line"><span class="attr">config_file_path :</span> <span class="string">./configfile/aba_config.yaml</span></span><br><span class="line"><span class="attr">extra_quantizer_dict:</span> </span><br><span class="line">   <span class="attr">special_layer:</span> </span><br><span class="line">      <span class="attr">input_layer :</span> <span class="number">10</span></span><br><span class="line">      <span class="attr">DBF.DBF_conv1.Conv1:</span> </span><br><span class="line">         <span class="attr">weight :</span> <span class="number">10</span> <span class="string">per_channel_affine</span></span><br><span class="line">         <span class="attr">activation :</span> <span class="number">10</span> <span class="string">per_tensor_affine</span></span><br><span class="line">      <span class="attr">DBF.DBF_conv1.lrelu:</span> </span><br><span class="line">         <span class="attr">weight :</span> <span class="number">10</span> <span class="string">per_channel_affine</span></span><br><span class="line">         <span class="attr">activation :</span> <span class="number">10</span> <span class="string">per_tensor_affine</span></span><br><span class="line">      <span class="attr">DBF.DBF_conv1.Conv2:</span> </span><br><span class="line">         <span class="attr">weight :</span> <span class="number">10</span> <span class="string">per_channel_affine</span></span><br><span class="line">         <span class="attr">activation :</span> <span class="number">10</span> <span class="string">per_tensor_affine</span></span><br><span class="line">      <span class="attr">DBF.DBF_conv9.Conv2:</span> </span><br><span class="line">         <span class="attr">weight :</span> <span class="number">10</span> <span class="string">per_channel_affine</span></span><br><span class="line">         <span class="attr">activation :</span> <span class="number">10</span> <span class="string">per_tensor_affine</span></span><br><span class="line">      <span class="attr">DBF.DBF_conv9.lrelu_dup1:</span> </span><br><span class="line">         <span class="attr">weight :</span> <span class="number">10</span> <span class="string">per_channel_affine</span></span><br><span class="line">         <span class="attr">activation :</span> <span class="number">10</span> <span class="string">per_tensor_affine</span></span><br><span class="line">      <span class="attr">DBF.DBF_out:</span> </span><br><span class="line">         <span class="attr">weight :</span> <span class="number">10</span> <span class="string">per_channel_affine</span></span><br><span class="line">         <span class="attr">activation :</span> <span class="number">10</span> <span class="string">per_tensor_affine</span></span><br><span class="line">      <span class="attr">DBLE.color_conv1.Conv1:</span> </span><br><span class="line">         <span class="attr">weight :</span> <span class="number">10</span> <span class="string">per_channel_affine</span></span><br><span class="line">         <span class="attr">activation :</span> <span class="number">10</span> <span class="string">per_tensor_affine</span></span><br><span class="line">      <span class="attr">DBLE.color_conv1.lrelu:</span> </span><br><span class="line">         <span class="attr">weight :</span> <span class="number">10</span> <span class="string">per_channel_affine</span></span><br><span class="line">         <span class="attr">activation :</span> <span class="number">10</span> <span class="string">per_tensor_affine</span></span><br><span class="line">      <span class="attr">DBLE.color_conv1.Conv2:</span> </span><br><span class="line">         <span class="attr">weight :</span> <span class="number">10</span> <span class="string">per_channel_affine</span></span><br><span class="line">         <span class="attr">activation :</span> <span class="number">10</span> <span class="string">per_tensor_affine</span></span><br><span class="line">      <span class="attr">DBLE.mono_conv1.Conv1:</span> </span><br><span class="line">         <span class="attr">weight :</span> <span class="number">10</span> <span class="string">per_channel_affine</span></span><br><span class="line">         <span class="attr">activation :</span> <span class="number">10</span> <span class="string">per_tensor_affine</span></span><br><span class="line">      <span class="attr">DBLE.mono_conv1.lrelu:</span> </span><br><span class="line">         <span class="attr">weight :</span> <span class="number">10</span> <span class="string">per_channel_affine</span></span><br><span class="line">         <span class="attr">activation :</span> <span class="number">10</span> <span class="string">per_tensor_affine</span></span><br><span class="line">      <span class="attr">DBLE.mono_conv1.Conv2:</span> </span><br><span class="line">         <span class="attr">weight :</span> <span class="number">10</span> <span class="string">per_channel_affine</span></span><br><span class="line">         <span class="attr">activation :</span> <span class="number">10</span> <span class="string">per_tensor_affine</span></span><br><span class="line">      <span class="attr">DBLE.dual_conv9.Conv1:</span> </span><br><span class="line">         <span class="attr">weight :</span> <span class="number">10</span> <span class="string">per_channel_affine</span></span><br><span class="line">         <span class="attr">activation :</span> <span class="number">10</span> <span class="string">per_tensor_affine</span></span><br><span class="line">      <span class="attr">DBLE.dual_conv9.lrelu:</span> </span><br><span class="line">         <span class="attr">weight :</span> <span class="number">10</span> <span class="string">per_channel_affine</span></span><br><span class="line">         <span class="attr">activation :</span> <span class="number">10</span> <span class="string">per_tensor_affine</span></span><br><span class="line">      <span class="attr">DBLE.dual_conv9.Conv2:</span> </span><br><span class="line">         <span class="attr">weight :</span> <span class="number">10</span> <span class="string">per_channel_affine</span></span><br><span class="line">         <span class="attr">activation :</span> <span class="number">10</span> <span class="string">per_tensor_affine</span></span><br><span class="line">      <span class="attr">DBLE.dual_conv9.lrelu_dup1:</span> </span><br><span class="line">         <span class="attr">activation :</span> <span class="number">10</span> <span class="string">per_tensor_affine</span></span><br><span class="line">      <span class="attr">DBLE.DBLE_out:</span> </span><br><span class="line">         <span class="attr">weight :</span> <span class="number">10</span> <span class="string">per_channel_affine</span></span><br><span class="line">         <span class="attr">activation :</span> <span class="number">10</span> <span class="string">per_tensor_affine</span></span><br><span class="line"><span class="attr">output_layer_list :</span> [<span class="string">&#x27;DBLE.dual_conv9.lrelu_dup1&#x27;</span>, <span class="string">&#x27;DBF.DBF_conv9.lrelu_dup1&#x27;</span>]</span><br><span class="line"><span class="attr">distill_config :</span> [<span class="string">&#x27;DBLE.dual_conv9.lrelu_dup1&#x27;</span>, <span class="string">&#x27;DBF.DBF_conv9.lrelu_dup1&#x27;</span>]</span><br><span class="line"><span class="attr">distill_coeff :</span> [<span class="number">0.7</span>, <span class="number">0.7</span>]</span><br></pre></td></tr></table></figure> ### Fri Oct 20 09:41:04 HKT 2023</li>
<li>启动了一个没有蒸馏的实验,剩余实验正在修改代码</li>
</ol>
<h3 id="wed-oct-24-104548-hkt-2023">Wed Oct 24 10:45:48 HKT 2023</h3>
<p>启动了一个蒸馏加权系数小一点的网络，发现权重越小效果越好,这会导致开始的情况不是那么坏</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="上一页" aria-label="上一页" href="/default-index/page/7/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/default-index/">1</a><span class="space">&hellip;</span><a class="page-number" href="/default-index/page/7/">7</a><span class="page-number current">8</span>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2023 – 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">skk1faker</span>
  </div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="utterances" type="application/json">{"enable":true,"repo":"skk1faker/blog_source","issue_term":"pathname","theme":"github-dark"}</script>
<script src="/js/third-party/comments/utterances.js"></script>

</body>
</html>
